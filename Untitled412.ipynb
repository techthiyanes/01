{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7LEfxMeEtKq3ncinSGSIA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/techthiyanes/01/blob/main/Untitled412.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWal57AIRZLv"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "import json\n",
        "from typing import Any, List, Mapping, Optional\n",
        "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain_core.language_models.llms import LLM\n",
        "from llama_index.llms.langchain import LangChainLLM\n",
        "import uuid\n",
        "\n",
        "class GPTV4(LLM):\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        backend='gpt-4',\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "\n",
        "\n",
        "        return response\n",
        "\n",
        "\n",
        "class GPTV35(LLM):\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"GPT\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        backend='gpt-4',\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "\n",
        "\n",
        "        return response\n",
        "\n",
        "def retrieve_llm():\n",
        "    lc_llmv4 = GPTV4()\n",
        "    lc_llmv35 = GPTV35()\n",
        "    lc_llm = lc_llmv4.with_fallbacks([lc_llmv35])\n",
        "    llm = LangChainLLM(llm=lc_llmv4)\n",
        "    return lc_llmv4,lc_llmv35,lc_llm,llm\n",
        "\n",
        "\n",
        "import requests\n",
        "from typing import List\n",
        "from langchain_core.embeddings import Embeddings\n",
        "import uuid\n",
        "import json\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "\n",
        "class CustomAPIEmbeddings(Embeddings):\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        text_embed = []\n",
        "        for i in range(0,len(texts)):\n",
        "            print(i)\n",
        "            print(texts[i])\n",
        "\n",
        "\n",
        "            token_response = requests.post(token_url, headers=token_headers)\n",
        "            token = token_response.json()[\"token_type\"] + \" \" + token_response.json()[\"access_token\"]\n",
        "            #\n",
        "            api_headers =  {\"Content-Type\":\"application/json\",\n",
        "                            \"ApiVersion\":\"1\",\n",
        "                            \"Authorization\":token}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            response = requests.post(url, headers=api_headers, data=json.dumps(inputs))\n",
        "            response_json = response.json()\n",
        "            text_embed.append(response_json['data'][0]['embedding'])\n",
        "        return text_embed\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        return self.embed_documents([text])[0]\n",
        "\n",
        "\n",
        "def retrieve_embeddings():\n",
        "    lc_embeddings = CustomAPIEmbeddings(model_name='api_openai_text_embedding_ada_002')\n",
        "    embed_model = LangchainEmbedding(lc_embeddings)\n",
        "    return embed_model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from llama_index.llms.langchain import LangChainLLM\n",
        "# from embeddings import CustomAPIEmbeddings\n",
        "# from sql_database import SQLDatabase\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from llama_index.core import Settings\n",
        "# from fetch_examples import fetch_examples\n",
        "from llama_index.core import PromptTemplate\n",
        "# import gradio as gr\n",
        "from loguru import logger\n",
        "\n",
        "logger.info(\"Initialize variables\")\n",
        "include_tables = ['']\n",
        "lc_llmv4,lc_llmv35,lc_llm,llm = retrieve_llm()\n",
        "embed_model = retrieve_embeddings()\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "logger.info(\"loading db\")\n",
        "\n",
        "#  Identify semantic matches of the tables\n",
        "\n",
        "db = SQLDatabase.from_uri(f'dummy_uri',\n",
        "                          include_tables = include_tables,\n",
        "                        #   ignore_tables = ignore_tables,\n",
        "                          sample_rows_in_table_info = 1,\n",
        "                          )\n",
        "logger.info(\"loaded db\")\n",
        "examples = fetch_examples()\n",
        "TABLE_PREFIX = \"\"\n",
        "prefix_template = '''\n",
        "            Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\n",
        "            You can order the results by a relevant column to return the most interesting examples in the database.\\n\\n\n",
        "            Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\\n\\n\n",
        "            Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist.\n",
        "            Pay attention to which column is in which table.\n",
        "            Also, qualify column names with the table name when needed.\n",
        "            Please consider below few shot examples while generating the query:\n",
        "            {few_shot_examples}\n",
        "'''\n",
        "suffix_template='''\n",
        "            While generating the query please conside below pre-requisites:\n",
        "            1) All table name will be prefixed with {table_prefix} e.g if table name is  you will return in query\n",
        "            2) Generate only snowflake sql\n",
        "            3) Database is case sensitive. For comparison use values from revelent question-sql pairs\n",
        "            4) Use insurance expertise to help understand the tables and contexts\n",
        "            5) Use only columns and tables present in the below info\n",
        "            6) Join tables where necessary.\n",
        "            7) Use fuzzy matching when text/string is considered. like %keyword%\n",
        "            8) If the query is already provided to you in the prompt use that and dont hallucinate.\n",
        "            9) Use SCI in capital letters\n",
        "            10) If the question is related to agent it refers to 'prodcodename' column in database.\n",
        "            11) Always search for the relevant\n",
        "            12) Submissions received have a status of \"Quote\" while submissions converted have a status of \"\". These are case sensitive\n",
        "            13) Capitalise table names as this is case senstive like\n",
        "            You are required to use the following format, each taking one line:\\n\\n\n",
        "            Question: Question here\\nSQLQuery: SQL Query to run\\n\n",
        "            SQLResult: Result of the SQLQuery\\n\n",
        "            Answer: Final answer here\\n\\n\n",
        "            Only use tables listed below.\\n{schema}\\n\\n\n",
        "            Question: {query_str}\\n\n",
        "            SQLQuery: '''\n",
        "query_engine = NLSQLTableQueryEngine(\n",
        "    sql_database=db, tables=include_tables,\n",
        "    llm=llm\n",
        ")\n",
        "schema = db.table_info\n",
        "logger.info(\"loaded variables\")\n",
        "\n",
        "# Append semantic column mapping as an inclusion to prompt\n",
        "# Pay attention to the table of <TABLE_NAME> on below columns..."
      ],
      "metadata": {
        "id": "KJLD4vHZRlUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_db_prefix(**kwargs):\n",
        "    return \"\"\n",
        "\n",
        "def semantic_examples(**kwargs):\n",
        "    query_str = kwargs[\"query_str\"]\n",
        "    corpus = []\n",
        "    for i in range(0,len(examples)):\n",
        "        corpus.append(examples[i]['input'])\n",
        "    embedder = SentenceTransformer(r\"\")\n",
        "    corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
        "    queries = [query_str]\n",
        "    example_match = []\n",
        "    for query in queries:\n",
        "        query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "        hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
        "        hits = hits[0]\n",
        "        for hit in hits:\n",
        "            inner_dict = {}\n",
        "            inner_dict[\"input\"] = corpus[hit['corpus_id']]\n",
        "            for i in range(0,len(examples)):\n",
        "                if corpus[hit['corpus_id']] in examples[i]['input']:\n",
        "                    inner_dict[\"query\"] = examples[i]['query']\n",
        "            example_match.append(inner_dict)\n",
        "    return '\\n'.join(map(str, example_match)).replace(\"{\",\"\").replace(\"}\",\"\")"
      ],
      "metadata": {
        "id": "FvFCwFQ_Rycl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sql_query_fn(query_str):\n",
        "# query_str = \"List down all tables loaded into the schema\"\n",
        "    logger.info(query_str)\n",
        "    template = prefix_template + suffix_template\n",
        "    txt_sql_prompt_tmpl = PromptTemplate(template,\n",
        "                                        function_mappings={\"few_shot_examples\": semantic_examples,\n",
        "                                                        \"table_prefix\": return_db_prefix}\n",
        "                                        )\n",
        "\n",
        "    query_engine.update_prompts(\n",
        "        {\"sql_retriever:text_to_sql_prompt\": txt_sql_prompt_tmpl}\n",
        "    )\n",
        "    logger.info(\"prompt****\"+str(query_engine.get_prompts()))\n",
        "    response = query_engine.query(query_str)\n",
        "    sql_query = response.metadata['sql_query']\n",
        "    return sql_query"
      ],
      "metadata": {
        "id": "ZV2ScJArR3Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sql_query_fn(\"List down all tables loaded into the schema\")"
      ],
      "metadata": {
        "id": "W6oHkugVR6XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def sql_query_tool_fn(query:str) -> str:\n",
        "    return sql_query_fn(query)\n",
        "from llama_index.core.agent import ReActAgent\n",
        "# sql_query_tool = FunctionTool.from_defaults(fn=sql_query_tool_fn)\n",
        "sql_query_tool = FunctionTool.from_defaults(fn=sql_query_fn)\n",
        "\n",
        "tools = [sql_query_tool]\n",
        "agent = ReActAgent.from_tools(tools, llm=llm, verbose=True)"
      ],
      "metadata": {
        "id": "OxdLajN_R6Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.chat(\"List down all tables loaded into the  schema\")"
      ],
      "metadata": {
        "id": "vqTNNwcgR6RI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.sources"
      ],
      "metadata": {
        "id": "bYF4goz3R6O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SQLDatabase\n",
        "from sqlalchemy import (\n",
        "    create_engine,\n",
        "    MetaData,\n",
        "    Table,\n",
        "    Column,\n",
        "    String,\n",
        "    Integer,\n",
        "    select,\n",
        "    column,\n",
        ")\n",
        "sql_database = db\n",
        "from llama_index.core.query_pipeline import QueryPipeline\n",
        "# define global callback setting\n",
        "from llama_index.core.settings import Settings\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "callback_manager = CallbackManager()\n",
        "Settings.callback_manager = callback_manager\n",
        "import llama_index.core\n",
        "sql_query_engine = query_engine\n",
        "sql_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=sql_query_engine,\n",
        "    name=\"sql_tool\",\n",
        "    description=(\n",
        "        \"Useful for translating a natural language query into a SQL query\"\n",
        "    ),\n",
        ")\n",
        "from llama_index.core.query_pipeline import QueryPipeline as QP\n",
        "qp = QP(verbose=True)\n",
        "from llama_index.core.agent.react.types import (\n",
        "    ActionReasoningStep,\n",
        "    ObservationReasoningStep,\n",
        "    ResponseReasoningStep,\n",
        ")\n",
        "from llama_index.core.agent import Task, AgentChatResponse\n",
        "from llama_index.core.query_pipeline import (\n",
        "    AgentInputComponent,\n",
        "    AgentFnComponent,\n",
        "    CustomAgentComponent,\n",
        "    QueryComponent,\n",
        "    ToolRunnerComponent,\n",
        ")\n",
        "from llama_index.core.llms import MessageRole\n",
        "from typing import Dict, Any, Optional, Tuple, List, cast\n",
        "\n",
        "\n",
        "## Agent Input Component\n",
        "## This is the component that produces agent inputs to the rest of the components\n",
        "## Can also put initialization logic here.\n",
        "def agent_input_fn(task: Task, state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"Agent input function.\n",
        "\n",
        "    Returns:\n",
        "        A Dictionary of output keys and values. If you are specifying\n",
        "        src_key when defining links between this component and other\n",
        "        components, make sure the src_key matches the specified output_key.\n",
        "\n",
        "    \"\"\"\n",
        "    # initialize current_reasoning\n",
        "    if \"current_reasoning\" not in state:\n",
        "        state[\"current_reasoning\"] = []\n",
        "    reasoning_step = ObservationReasoningStep(observation=task.input)\n",
        "    state[\"current_reasoning\"].append(reasoning_step)\n",
        "    return {\"input\": task.input}\n",
        "\n",
        "\n",
        "agent_input_component = AgentInputComponent(fn=agent_input_fn)\n",
        "\n",
        "from llama_index.core.agent import ReActChatFormatter\n",
        "from llama_index.core.query_pipeline import InputComponent, Link\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.tools import BaseTool\n",
        "\n",
        "\n",
        "## define prompt function\n",
        "def react_prompt_fn(\n",
        "    task: Task, state: Dict[str, Any], input: str, tools: List[BaseTool]\n",
        ") -> List[ChatMessage]:\n",
        "    # Add input to reasoning\n",
        "    chat_formatter = ReActChatFormatter()\n",
        "    return chat_formatter.format(\n",
        "        tools,\n",
        "        chat_history=task.memory.get() + state[\"memory\"].get_all(),\n",
        "        current_reasoning=state[\"current_reasoning\"],\n",
        "    )\n",
        "\n",
        "\n",
        "react_prompt_component = AgentFnComponent(\n",
        "    fn=react_prompt_fn, partial_dict={\"tools\": [sql_tool]}\n",
        ")\n",
        "\n",
        "from typing import Set, Optional\n",
        "from llama_index.core.agent.react.output_parser import ReActOutputParser\n",
        "from llama_index.core.llms import ChatResponse\n",
        "from llama_index.core.agent.types import Task\n",
        "\n",
        "\n",
        "def parse_react_output_fn(\n",
        "    task: Task, state: Dict[str, Any], chat_response: ChatResponse\n",
        "):\n",
        "    \"\"\"Parse ReAct output into a reasoning step.\"\"\"\n",
        "    output_parser = ReActOutputParser()\n",
        "    reasoning_step = output_parser.parse(chat_response.message.content)\n",
        "    return {\"done\": reasoning_step.is_done, \"reasoning_step\": reasoning_step}\n",
        "\n",
        "\n",
        "parse_react_output = AgentFnComponent(fn=parse_react_output_fn)\n",
        "\n",
        "\n",
        "def run_tool_fn(\n",
        "    task: Task, state: Dict[str, Any], reasoning_step: ActionReasoningStep\n",
        "):\n",
        "    \"\"\"Run tool and process tool output.\"\"\"\n",
        "    tool_runner_component = ToolRunnerComponent(\n",
        "        [sql_tool], callback_manager=task.callback_manager\n",
        "    )\n",
        "    tool_output = tool_runner_component.run_component(\n",
        "        tool_name=reasoning_step.action,\n",
        "        tool_input=reasoning_step.action_input,\n",
        "    )\n",
        "    observation_step = ObservationReasoningStep(observation=str(tool_output))\n",
        "    state[\"current_reasoning\"].append(observation_step)\n",
        "    # TODO: get output\n",
        "\n",
        "    return {\"response_str\": observation_step.get_content(), \"is_done\": False}\n",
        "\n",
        "\n",
        "run_tool = AgentFnComponent(fn=run_tool_fn)\n",
        "\n",
        "\n",
        "def process_response_fn(\n",
        "    task: Task, state: Dict[str, Any], response_step: ResponseReasoningStep\n",
        "):\n",
        "    \"\"\"Process response.\"\"\"\n",
        "    state[\"current_reasoning\"].append(response_step)\n",
        "    response_str = response_step.response\n",
        "    # Now that we're done with this step, put into memory\n",
        "    state[\"memory\"].put(ChatMessage(content=task.input, role=MessageRole.USER))\n",
        "    state[\"memory\"].put(\n",
        "        ChatMessage(content=response_str, role=MessageRole.ASSISTANT)\n",
        "    )\n",
        "\n",
        "    return {\"response_str\": response_str, \"is_done\": True}\n",
        "\n",
        "\n",
        "process_response = AgentFnComponent(fn=process_response_fn)\n",
        "\n",
        "\n",
        "def process_agent_response_fn(\n",
        "    task: Task, state: Dict[str, Any], response_dict: dict\n",
        "):\n",
        "    \"\"\"Process agent response.\"\"\"\n",
        "    return (\n",
        "        AgentChatResponse(response_dict[\"response_str\"]),\n",
        "        response_dict[\"is_done\"],\n",
        "    )\n",
        "\n",
        "\n",
        "process_agent_response = AgentFnComponent(fn=process_agent_response_fn)\n",
        "\n",
        "from llama_index.core.query_pipeline import QueryPipeline as QP\n",
        "from llama_index.llms.openai import OpenAI\n",
        "\n",
        "qp.add_modules(\n",
        "    {\n",
        "        \"agent_input\": agent_input_component,\n",
        "        \"react_prompt\": react_prompt_component,\n",
        "        \"llm\": llm,\n",
        "        \"react_output_parser\": parse_react_output,\n",
        "        \"run_tool\": run_tool,\n",
        "        \"process_response\": process_response,\n",
        "        \"process_agent_response\": process_agent_response,\n",
        "    }\n",
        ")\n",
        "# link input to react prompt to parsed out response (either tool action/input or observation)\n",
        "qp.add_chain([\"agent_input\", \"react_prompt\", \"llm\", \"react_output_parser\"])\n",
        "\n",
        "# add conditional link from react output to tool call (if not done)\n",
        "qp.add_link(\n",
        "    \"react_output_parser\",\n",
        "    \"run_tool\",\n",
        "    condition_fn=lambda x: not x[\"done\"],\n",
        "    input_fn=lambda x: x[\"reasoning_step\"],\n",
        ")\n",
        "# add conditional link from react output to final response processing (if done)\n",
        "qp.add_link(\n",
        "    \"react_output_parser\",\n",
        "    \"process_response\",\n",
        "    condition_fn=lambda x: x[\"done\"],\n",
        "    input_fn=lambda x: x[\"reasoning_step\"],\n",
        ")\n",
        "\n",
        "# whether response processing or tool output processing, add link to final agent response\n",
        "qp.add_link(\"process_response\", \"process_agent_response\")\n",
        "qp.add_link(\"run_tool\", \"process_agent_response\")\n",
        "\n",
        "# from pyvis.network import Network\n",
        "\n",
        "# net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
        "# net.from_nx(qp.clean_dag)\n",
        "# net.show(\"agent_dag.html\")\n",
        "\n",
        "from llama_index.core.agent import QueryPipelineAgentWorker\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "\n",
        "agent_worker = QueryPipelineAgentWorker(qp)\n",
        "agent = agent_worker.as_agent(\n",
        "    callback_manager=CallbackManager([]), verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "dGeTlP57SKgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# start task\n",
        "task = agent.create_task(\n",
        "    \"List all tables from this schema\"\n",
        ")\n",
        "step_output = agent.run_step(task.task_id)"
      ],
      "metadata": {
        "id": "hCpup5HzSKcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent import Task, AgentChatResponse\n",
        "from typing import Dict, Any\n",
        "from llama_index.core.query_pipeline import (\n",
        "    AgentInputComponent,\n",
        "    AgentFnComponent,\n",
        ")\n",
        "\n",
        "\n",
        "def agent_input_fn(task: Task, state: Dict[str, Any]) -> Dict:\n",
        "    \"\"\"Agent input function.\"\"\"\n",
        "    # initialize current_reasoning\n",
        "    if \"convo_history\" not in state:\n",
        "        state[\"convo_history\"] = []\n",
        "        state[\"count\"] = 0\n",
        "    state[\"convo_history\"].append(f\"User: {task.input}\")\n",
        "    convo_history_str = \"\\n\".join(state[\"convo_history\"]) or \"None\"\n",
        "    return {\"input\": task.input, \"convo_history\": convo_history_str}\n",
        "\n",
        "\n",
        "agent_input_component = AgentInputComponent(fn=agent_input_fn)\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "retry_prompt_str = \"\"\"\\\n",
        "You are trying to generate a proper natural language query given a user input.\n",
        "\n",
        "This query will then be interpreted by a downstream text-to-SQL agent which\n",
        "will convert the query to a SQL statement. If the agent triggers an error,\n",
        "then that will be reflected in the current conversation history (see below).\n",
        "\n",
        "If the conversation history is None, use the user input. If its not None,\n",
        "generate a new SQL query that avoids the problems of the previous SQL query.\n",
        "\n",
        "Input: {input}\n",
        "Convo history (failed attempts):\n",
        "{convo_history}\n",
        "\n",
        "New input: \"\"\"\n",
        "retry_prompt = PromptTemplate(retry_prompt_str)\n",
        "from llama_index.core import Response\n",
        "from typing import Tuple\n",
        "\n",
        "# validate_prompt_str = \"\"\"\\\n",
        "# Given the user query, validate whether the inferred SQL query and response from executing the query is correct and answers the query.\n",
        "\n",
        "# Answer with YES or NO.\n",
        "\n",
        "# Query: {input}\n",
        "# Inferred SQL query: {sql_query}\n",
        "# SQL Response: {sql_response}\n",
        "\n",
        "# Result: \"\"\"\n",
        "\n",
        "validate_prompt_str = \"\"\"\\\n",
        "Given the user query, validate whether the inferred SQL query and response from executing the query is correct and answers the query.\n",
        "\n",
        "Answer with YES or NO.\n",
        "\n",
        "Query: {input}\n",
        "Inferred SQL query: {sql_query}\n",
        "\n",
        "Result: \"\"\"\n",
        "\n",
        "validate_prompt = PromptTemplate(validate_prompt_str)\n",
        "\n",
        "MAX_ITER = 3\n",
        "\n",
        "\n",
        "def agent_output_fn(\n",
        "    task: Task, state: Dict[str, Any], output: Response\n",
        ") -> Tuple[AgentChatResponse, bool]:\n",
        "    \"\"\"Agent output component.\"\"\"\n",
        "    print(f\"> Inferred SQL Query: {output.metadata['sql_query']}\")\n",
        "    # print(f\"> SQL Response: {str(output)}\")\n",
        "    state[\"convo_history\"].append(\n",
        "        f\"Assistant (inferred SQL query): {output.metadata['sql_query']}\"\n",
        "    )\n",
        "    # state[\"convo_history\"].append(f\"Assistant (response): {str(output)}\")\n",
        "\n",
        "    # run a mini chain to get response\n",
        "    validate_prompt_partial = validate_prompt.as_query_component(\n",
        "        partial={\n",
        "            \"sql_query\": output.metadata[\"sql_query\"],\n",
        "            # \"sql_response\": str(output),\n",
        "        }\n",
        "    )\n",
        "    qp = QP(chain=[validate_prompt_partial, llm])\n",
        "    validate_output = qp.run(input=task.input)\n",
        "\n",
        "    state[\"count\"] += 1\n",
        "    is_done = False\n",
        "    if state[\"count\"] >= MAX_ITER:\n",
        "        is_done = True\n",
        "    # if \"YES\" in validate_output.message.content:\n",
        "    #     is_done = True\n",
        "\n",
        "    return AgentChatResponse(response=str(output)), is_done\n",
        "\n",
        "\n",
        "agent_output_component = AgentFnComponent(fn=agent_output_fn)\n",
        "from llama_index.core.query_pipeline import (\n",
        "    QueryPipeline as QP,\n",
        "    Link,\n",
        "    InputComponent,\n",
        ")\n",
        "\n",
        "qp = QP(\n",
        "    modules={\n",
        "        \"input\": agent_input_component,\n",
        "        \"retry_prompt\": retry_prompt,\n",
        "        \"llm\": llm,\n",
        "        \"sql_query_engine\": sql_query_engine,\n",
        "        \"output_component\": agent_output_component,\n",
        "    },\n",
        "    verbose=True,\n",
        ")\n",
        "qp.add_link(\"input\", \"retry_prompt\", src_key=\"input\", dest_key=\"input\")\n",
        "qp.add_link(\n",
        "    \"input\", \"retry_prompt\", src_key=\"convo_history\", dest_key=\"convo_history\"\n",
        ")\n",
        "qp.add_chain([\"retry_prompt\", \"llm\", \"sql_query_engine\", \"output_component\"])\n",
        "from llama_index.core.agent import QueryPipelineAgentWorker\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "\n",
        "# callback manager is passed from query pipeline to agent worker/agent\n",
        "agent_worker = QueryPipelineAgentWorker(qp)\n",
        "agent = agent_worker.as_agent(\n",
        "    callback_manager=CallbackManager(), verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "8zlTmZvWSKaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.chat(\n",
        "    \"List all tables from schema\"\n",
        ")\n",
        "print(str(response))"
      ],
      "metadata": {
        "id": "8lorxwEQSKXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "import json\n",
        "from typing import Any, List, Mapping, Optional\n",
        "from langchain_core.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain_core.language_models.llms import LLM\n",
        "from llama_index.llms.langchain import LangChainLLM\n",
        "import uuid\n",
        "\n",
        "class GPTV4(LLM):\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"GPT\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        backend='gpt-4',\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "\n",
        "\n",
        "        return response\n",
        "\n",
        "\n",
        "class GPTV35(LLM):\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"GPT\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        stop: Optional[List[str]] = None,\n",
        "        backend='gpt-4',\n",
        "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "\n",
        "\n",
        "        return response\n",
        "\n",
        "def retrieve_llm():\n",
        "    lc_llmv4 = GPTV4()\n",
        "    lc_llmv35 = GPTV35()\n",
        "    lc_llm = lc_llmv4.with_fallbacks([lc_llmv35])\n",
        "    llm = LangChainLLM(llm=lc_llmv4)\n",
        "    return lc_llmv4,lc_llmv35,lc_llm,llm\n",
        "\n",
        "\n",
        "import requests\n",
        "from typing import List\n",
        "from langchain_core.embeddings import Embeddings\n",
        "import uuid\n",
        "import json\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "\n",
        "class CustomAPIEmbeddings(Embeddings):\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model_name = model_name\n",
        "\n",
        "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
        "        text_embed = []\n",
        "        for i in range(0,len(texts)):\n",
        "        return text_embed\n",
        "\n",
        "    def embed_query(self, text: str) -> List[float]:\n",
        "        return self.embed_documents([text])[0]\n",
        "\n",
        "\n",
        "def retrieve_embeddings():\n",
        "    lc_embeddings = CustomAPIEmbeddings(model_name='api_openai_text_embedding_ada_002')\n",
        "    embed_model = LangchainEmbedding(lc_embeddings)\n",
        "    return embed_model,lc_embeddings\n"
      ],
      "metadata": {
        "id": "6V6wVoRmSKVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "lc_llmv4,lc_llmv35,lc_llm,llm = retrieve_llm()\n",
        "embed_model,_ = retrieve_embeddings()\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "chain = ConversationalRetrievalChain.from_llm(\n",
        "        lc_llmv4,\n",
        "        retriever,\n",
        "        return_source_documents=True,\n",
        "        combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
        "    )\n",
        "\n",
        "query = \"summarize the content\"\n",
        "result = chain({\"question\": query,\n",
        "                \"chat_history\":[],},return_only_outputs=False,)\n",
        "result[\"answer\"]\n",
        "\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "topk_context = []\n",
        "mat_doc = retriever.get_relevant_documents(query)\n",
        "for i in range(0,len(mat_doc)):\n",
        "    topk_context.append(mat_doc[i].page_content)\n",
        "\n",
        "system_prompt = \"You are a helpful assistant and diligent Question & Answering Engine that accurately answers the user's queries based on the given context\"\n",
        "prompt = ChatPromptTemplate.from_template(system_prompt+\"\\n\"+prompt_template)\n",
        "chain = LLMChain(\n",
        "                llm=lc_llmv4,\n",
        "                prompt=prompt,\n",
        "                output_parser=StrOutputParser())\n",
        "context = ''.join(map(str, topk_context))\n",
        "context\n",
        "chain.run(context = context, question = query)"
      ],
      "metadata": {
        "id": "cNylAVH3Sy14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "def fetch_examples():\n",
        "    df_examples = pd.read_excel(r\"C:\\Users\\THM0001\\Downloads\\Examples.xlsx\")\n",
        "    queries = list(df_examples['query'].values)\n",
        "    queries = [re.sub(r'\\n', ' ', i) for i in queries]\n",
        "    queries = [re.sub(r'\\xa0', ' ', i) for i in queries]\n",
        "    queries\n",
        "    inputs = list(df_examples[\"input\"].values)\n",
        "    examples = []\n",
        "    for j in range(0,len(queries)):\n",
        "        example_dict = {}\n",
        "        example_dict[\"input\"] = inputs[j]\n",
        "        example_dict[\"query\"] = queries[j]\n",
        "        examples.append(example_dict)\n",
        "    return examples\n",
        "\n",
        "\n",
        "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from llama_index.llms.langchain import LangChainLLM\n",
        "# from _gpt import GPT\n",
        "# from embeddings import CustomAPIEmbeddings\n",
        "# from sql_database import SQLDatabase\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from llama_index.core import Settings\n",
        "# from fetch_examples import fetch_examples\n",
        "from llama_index.core import PromptTemplate\n",
        "# import gradio as gr\n",
        "from loguru import logger\n",
        "\n",
        "\n",
        "def semantic_examples(query_str):\n",
        "    examples = fetch_examples()\n",
        "    corpus = []\n",
        "    for i in range(0,len(examples)):\n",
        "        corpus.append(examples[i]['input'])\n",
        "    embedder = SentenceTransformer(r\"C:\\Cacasi\\cacasci\\src\\policy_enhancements\\all-MiniLM-L6-v2\")\n",
        "    corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
        "    queries = [query_str]\n",
        "    example_match = []\n",
        "    for query in queries:\n",
        "        query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "        hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
        "        hits = hits[0]\n",
        "        for hit in hits:\n",
        "            inner_dict = {}\n",
        "            inner_dict[\"input\"] = corpus[hit['corpus_id']]\n",
        "            for i in range(0,len(examples)):\n",
        "                if corpus[hit['corpus_id']] in examples[i]['input']:\n",
        "                    inner_dict[\"query\"] = examples[i]['query']\n",
        "            example_match.append(inner_dict)\n",
        "    return '\\n'.join(map(str, example_match)).replace(\"{\",\"\").replace(\"}\",\"\")\n",
        "\n",
        "\n",
        "logger.info(\"Initialize variables\")\n",
        "include_tables = ['citdm321_pol_renl', 'citdm341_pol_inf', 'citdm271_pol_new_busn']\n",
        "lc_llmv4,lc_llmv35,lc_llm,llm = retrieve_llm()\n",
        "# embed_model = retrieve_embeddings()\n",
        "# Settings.llm = llm\n",
        "# Settings.embed_model = embed_model\n",
        "logger.info(\"loading db\")\n",
        "\n",
        "#  Identify semantic matches of the tables\n",
        "\n",
        "db = SQLDatabase.from_uri(f'dummy_uri',\n",
        "                          include_tables = include_tables,\n",
        "                        #   ignore_tables = ignore_tables,\n",
        "                          sample_rows_in_table_info = 1,\n",
        "                          )\n",
        "logger.info(\"loaded db\")\n",
        "# examples = fetch_examples()\n",
        "# TABLE_PREFIX = \"\"\n",
        "\n",
        "\n",
        "def load_dialect():\n",
        "    return db.dialect\n",
        "\n",
        "def load_table_prefix():\n",
        "    return \"\"\n",
        "\n",
        "def load_listed_tables():\n",
        "    return db.table_info"
      ],
      "metadata": {
        "id": "sbA8K4UeTGiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sql_text_tool_fn(query:str)->str:\n",
        "    template1 = '''\n",
        "                Given an input question, first create a syntactically correct data dialect {dialect} query to run, then look at the results of the query and return the answer.\n",
        "                You can order the results by a relevant column to return the most interesting examples in the database.\\n\\n\n",
        "                Never query for all the columns from a specific table, only ask for a few relevant columns given the question.\\n\\n\n",
        "                Pay attention to use only the column names that you can see in the schema description. Be careful to not query for columns that do not exist.\n",
        "                Pay attention to which column is in which table.\n",
        "                Also, qualify column names with the table name when needed.\n",
        "                Please consider below few shot examples while generating the query:\n",
        "                '''.format(dialect = load_dialect())\n",
        "\n",
        "    template2 = \"{few_shot_examples}\".format(few_shot_examples=semantic_examples(query))\n",
        "    template3 ='''\n",
        "                While generating the query please conside below pre-requisites:\n",
        "                1) All table name will be prefixed with {table_prefix} e.g if table name is  you will return . in query\n",
        "                2) Generate only snowflake sql\n",
        "                3) Database is case sensitive. For comparison use values from revelent question-sql pairs\n",
        "                4) Use insurance expertise to help understand the tables and contexts\n",
        "                5) Use only columns and tables present in the below info\n",
        "                6) Join tables where necessary.\n",
        "                7) Use fuzzy matching when text/string is considered. like %keyword%\n",
        "                8) If the query is already provided to you in the prompt use that and dont hallucinate.\n",
        "                9) Use SCI in capital letters\n",
        "                10) If the question is related to agent it refers to 'prodcodename' column in database.\n",
        "                11) Always search for the relevant\n",
        "                12) Submissions received have a status of \"Quote\" while submissions converted have a status of \"Inforce\". These are case sensitive\n",
        "                13) Capitalise table names as this is case senstive like\n",
        "                You are required to use the following format, each taking one line:\\n\\n\n",
        "                Only use tables listed below.\\n{schema}\\n\\n\n",
        "\n",
        "                '''.format(table_prefix = load_table_prefix(),\n",
        "                        schema = load_listed_tables())\n",
        "    template_last = '''\n",
        "            Question: {input}\n",
        "                SQLQuery:\n",
        "        '''\n",
        "\n",
        "    template4 ='''            You have access to the following tool: {tools} when you need to access the URI for a given concept. This is needed to generate the sparql query\n",
        "\n",
        "                Use the following format:\n",
        "\n",
        "                Question: the input question you must answer\n",
        "                Thought: you should always think about what to do\n",
        "                Action: the action to take, should be one of [{tool_names}]\n",
        "                Action Input: the input to the action\n",
        "                Observation: the result of the action. This should be a URI retrieved for the purpose of writing a sparql query\n",
        "                ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "                Thought: I now know the final answer\n",
        "                Final Answer: the final answer to the original input question.Always return SQL Query Alone as final Answer\n",
        "\n",
        "                Question: {input}\n",
        "                SQLQuery:\n",
        "                {agent_scratchpad}\n",
        "                '''\n",
        "\n",
        "    template5 ='''            You have access to the following tool: {tools} when you need to access the URI for a given concept. This is needed to generate the sparql query\n",
        "\n",
        "                Use the following format:\n",
        "\n",
        "                Question: the input question you must answer\n",
        "                Thought: you should always think about what to do\n",
        "                Action: the action to take, should be one of [{tool_names}]\n",
        "                Action Input: the input to the action\n",
        "                Observation: the result of the action. This should be a URI retrieved for the purpose of writing a sparql query\n",
        "                ... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "                Thought: I now know the final answer\n",
        "                Final Answer: the final answer to the original input question.Always return SQL Query Alone as final Answer\n",
        "                '''\n",
        "\n",
        "    template6 = '''\n",
        "\n",
        "    Begin!\n",
        "\n",
        "    Question: {input}\n",
        "    Thought:{agent_scratchpad}\"\"\"\n",
        "\n",
        "    '''\n",
        "\n",
        "    template = template1+template2+template3+template_last\n",
        "    # query_engine = NLSQLTableQueryEngine(\n",
        "    #     sql_database=db, tables=include_tables,\n",
        "    #     llm=llm\n",
        "    # )\n",
        "    # schema = db.table_info\n",
        "    # logger.info(\"loaded variables\")\n",
        "\n",
        "    # Append semantic column mapping as an inclusion to prompt\n",
        "    # Pay attention to the table of <TABLE_NAME> on below columns...\n",
        "    # template\n",
        "\n",
        "    # template_1 = template1+template2+template3\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_template(\"You are diiligent SQL Query generator engine where create SQL query alone as an output.\"+\"\\n\"+template)\n",
        "    chain = LLMChain(\n",
        "                    llm=lc_llmv4,\n",
        "                    prompt=prompt,\n",
        "                    output_parser=StrOutputParser())\n",
        "    return chain.run(input = query)\n",
        "sql_text_tool_fn(\"Load all tables from the schema\")\n",
        "\n",
        "\n",
        "def conversational_retrieval_chain_QA(query: str) -> str:\n",
        "    # \"this tool can be used when the user is asking queries around the document using conversational retrieval chain\"\n",
        "    # query = \"summarize the content\"\n",
        "    lc_llmv4,lc_llmv35,lc_llm,llm = retrieve_llm()\n",
        "    # embed_model,_ = retrieve_embeddings()\n",
        "\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "            lc_llmv4,\n",
        "            retriever,\n",
        "            return_source_documents=True,\n",
        "            combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
        "        )\n",
        "    result = chain({\"question\": query,\n",
        "                    \"chat_history\":[],},return_only_outputs=False,)\n",
        "    return result[\"answer\"]\n",
        "\n",
        "\n",
        "def LLM_chain_QA(query: str) -> str:\n",
        "    # \"this tool can be used when the user is asking queries around the document using LLM chain\"\n",
        "    topk_context = []\n",
        "    mat_doc = retriever.get_relevant_documents(query)\n",
        "    for i in range(0,len(mat_doc)):\n",
        "        topk_context.append(mat_doc[i].page_content)\n",
        "\n",
        "    system_prompt = \"You are a helpful assistant and diligent Question & Answering Engine that accurately answers the user's queries based on the given context\"\n",
        "    prompt = ChatPromptTemplate.from_template(system_prompt+\"\\n\"+prompt_template)\n",
        "    chain = LLMChain(\n",
        "                    llm=lc_llmv4,\n",
        "                    prompt=prompt,\n",
        "                    output_parser=StrOutputParser())\n",
        "    context = ''.join(map(str, topk_context))\n",
        "    context\n",
        "    return chain.run(context = context, question = query)\n",
        "\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "from llama_index.core.agent import ReActAgent\n",
        "conversational_retrieval_chain_QA_tool = FunctionTool.from_defaults(fn=conversational_retrieval_chain_QA)\n",
        "LLM_chain_QA_tool = FunctionTool.from_defaults(fn=LLM_chain_QA)\n",
        "\n",
        "\n",
        "\n",
        "# https://python.langchain.com/v0.1/docs/use_cases/tool_use/agents/\n",
        "\n",
        "\n",
        "def conversational_retrieval_chain_QA(query: str) -> str:\n",
        "    # \"this tool can be used when the user is asking queries around the document using conversational retrieval chain\"\n",
        "    # query = \"summarize the content\"\n",
        "    lc_llmv4,lc_llmv35,lc_llm,llm = retrieve_llm()\n",
        "    # embed_model,_ = retrieve_embeddings()\n",
        "\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    chain = ConversationalRetrievalChain.from_llm(\n",
        "            lc_llmv4,\n",
        "            retriever,\n",
        "            return_source_documents=True,\n",
        "            combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
        "        )\n",
        "    result = chain({\"question\": query,\n",
        "                    \"chat_history\":[],},return_only_outputs=False,)\n",
        "    return result[\"answer\"]\n",
        "\n",
        "\n",
        "def LLM_chain_QA(query: str) -> str:\n",
        "    # \"this tool can be used when the user is asking queries around the document using LLM chain\"\n",
        "    topk_context = []\n",
        "    mat_doc = retriever.get_relevant_documents(query)\n",
        "    for i in range(0,len(mat_doc)):\n",
        "        topk_context.append(mat_doc[i].page_content)\n",
        "\n",
        "    system_prompt = \"You are a helpful assistant and diligent Question & Answering Engine that accurately answers the user's queries based on the given context\"\n",
        "    prompt = ChatPromptTemplate.from_template(system_prompt+\"\\n\"+prompt_template)\n",
        "    chain = LLMChain(\n",
        "                    llm=lc_llmv4,\n",
        "                    prompt=prompt,\n",
        "                    output_parser=StrOutputParser())\n",
        "    context = ''.join(map(str, topk_context))\n",
        "    context\n",
        "    return chain.run(context = context, question = query)\n",
        "\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "from llama_index.core.agent import ReActAgent\n",
        "conversational_retrieval_chain_QA_tool = FunctionTool.from_defaults(fn=conversational_retrieval_chain_QA)\n",
        "LLM_chain_QA_tool = FunctionTool.from_defaults(fn=LLM_chain_QA)\n",
        "\n",
        "def apim_token_response():\n",
        "\n",
        "    apim_response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "    return apim_response\n",
        "\n",
        "def web_reponses(query:str):\n",
        "    apim_response = apim_token_response()\n",
        "\n",
        "    headers = {\n",
        "    'apiVersion': '1',\n",
        "    'Content-Type': 'application/json',\n",
        "    'Authorization': 'Bearer '+apim_response.json()['access_token']\n",
        "    }\n",
        "    web_response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "    return web_response\n",
        "\n",
        "def web_prompt_tool(query):\n",
        "    web_response = web_reponses(query)\n",
        "    print(web_response)\n",
        "    snippets = []\n",
        "    for i in range(0,len(web_response.json()['result']['webPages']['value'])):\n",
        "        val = web_response.json()['result']['webPages']['value'][i]\n",
        "        snippets.append(val['snippet'])\n",
        "    return \" \".join(snippets)\n",
        "    # return query+\" web tools\"\n",
        "web_prompt_tool(\"Open source LLM models\")\n",
        "\n",
        "def normal_regular_prompt(query):\n",
        "    return llm.complete(query)\n",
        "from llama_index.core.agent import ReActAgent\n",
        "website_prompt_tool = FunctionTool.from_defaults(fn=web_prompt_tool)\n",
        "normal_regular_prompt_tool = FunctionTool.from_defaults(fn=normal_regular_prompt)\n",
        "\n",
        "\n",
        "def web_normal_prompt_tool_fn(query:str)->str:\n",
        "    # Mined\n",
        "    # Final Code\n",
        "    def apim_token_response():\n",
        "\n",
        "        apim_response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "        return apim_response\n",
        "\n",
        "    def web_reponses(query:str):\n",
        "        apim_response = apim_token_response()\n",
        "\n",
        "        headers = {\n",
        "        'apiVersion': '1',\n",
        "        'Content-Type': 'application/json',\n",
        "        'Authorization': 'Bearer '+apim_response.json()['access_token']\n",
        "        }\n",
        "        web_response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "        return web_response\n",
        "\n",
        "    def web_prompt_tool(query):\n",
        "        web_response = web_reponses(query)\n",
        "        print(web_response)\n",
        "        snippets = []\n",
        "        for i in range(0,len(web_response.json()['result']['webPages']['value'])):\n",
        "            val = web_response.json()['result']['webPages']['value'][i]\n",
        "            snippets.append(val['snippet'])\n",
        "        return \" \".join(snippets)\n",
        "        # return query+\" web tools\"\n",
        "    web_prompt_tool(\"Open source LLM models\")\n",
        "\n",
        "\n",
        "    import logging\n",
        "    import sys\n",
        "    from azure.core.credentials import AzureKeyCredential\n",
        "    from azure.search.documents import SearchClient\n",
        "    from azure.search.documents.indexes import SearchIndexClient\n",
        "    from IPython.display import Markdown, display\n",
        "    from llama_index.core import (\n",
        "        SimpleDirectoryReader,\n",
        "        StorageContext,\n",
        "        VectorStoreIndex,\n",
        "    )\n",
        "    from llama_index.core.settings import Settings\n",
        "\n",
        "    from llama_index.llms.azure_openai import AzureOpenAI\n",
        "    from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
        "    from llama_index.vector_stores.azureaisearch import AzureAISearchVectorStore\n",
        "    from llama_index.vector_stores.azureaisearch import (\n",
        "        IndexManagement,\n",
        "        MetadataIndexFieldType,\n",
        "    )\n",
        "\n",
        "\n",
        "    from llama_index.agent.openai import OpenAIAgent\n",
        "    # from llama_index.core.tools.google import GmailToolSpec\n",
        "    from llama_index.core.tools import FunctionTool\n",
        "\n",
        "    def normal_regular_prompt(query):\n",
        "        return llm.complete(query)\n",
        "    from llama_index.core.agent import ReActAgent\n",
        "    function_tool1 = FunctionTool.from_defaults(fn=web_prompt_tool)\n",
        "    function_tool2 = FunctionTool.from_defaults(fn=normal_regular_prompt)\n",
        "    tools = [function_tool1,function_tool2]\n",
        "    agent = ReActAgent.from_tools(tools, llm=llm, verbose=True)\n",
        "    # response = agent.chat(\"Open source LLM models\")\n",
        "\n",
        "\n",
        "    from llama_index.core import PromptTemplate\n",
        "\n",
        "    PROMPT = \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    # Begin!\n",
        "\n",
        "    # New input: {input}\n",
        "    # {agent_scratchpad}\n",
        "    react_system_prompt = PromptTemplate(PROMPT)\n",
        "    agent.get_prompts()\n",
        "\n",
        "    agent.update_prompts({\"agent_worker:system_prompt\": react_system_prompt})\n",
        "    agent.reset()\n",
        "    response = agent.chat(\"text generation models from huggingface sources\")\n",
        "    print(response)\n",
        "    return response\n",
        "\n",
        "web_normal_prompt_tool = FunctionTool.from_defaults(fn=web_normal_prompt_tool_fn)\n",
        "sql_text_tool_fn\n",
        "\n",
        "sql_text_tool = FunctionTool.from_defaults(fn=sql_text_tool_fn)\n",
        "tools = [conversational_retrieval_chain_QA_tool,\n",
        "         LLM_chain_QA_tool,\n",
        "         website_prompt_tool,\n",
        "         normal_regular_prompt_tool,\n",
        "         web_normal_prompt_tool,\n",
        "         sql_text_tool,\n",
        "         ]\n",
        "agent = ReActAgent.from_tools(tools,\n",
        "                              llm=llm,\n",
        "                              max_iterations=3,\n",
        "                              verbose=True)\n",
        "\n",
        "response = agent.chat(\"what are the sections mentioned in the document of Animal Bailee Coverage as per LLM chain along with citations?\")"
      ],
      "metadata": {
        "id": "R7kG5zwkTGek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def conversational_retrieval_chain_QA(query: str) -> str:\n",
        "    \"this tool can be used when the user is asking queries around the document using conversational retrieval chain\"\n",
        "    # query = \"summarize the content\"\n",
        "    result = chain({\"question\": query,\n",
        "                    \"chat_history\":[],},return_only_outputs=False,)\n",
        "    return result[\"answer\"]\n",
        "\n",
        "\n",
        "@tool\n",
        "def LLM_chain_QA(query: str) -> str:\n",
        "    \"this tool can be used when the user is asking queries around the document using LLM chain\"\n",
        "    topk_context = []\n",
        "    mat_doc = retriever.get_relevant_documents(query)\n",
        "    for i in range(0,len(mat_doc)):\n",
        "        topk_context.append(mat_doc[i].page_content)\n",
        "\n",
        "    system_prompt = \"You are a helpful assistant and diligent Question & Answering Engine that accurately answers the user's queries based on the given context\"\n",
        "    prompt = ChatPromptTemplate.from_template(system_prompt+\"\\n\"+prompt_template)\n",
        "    chain = LLMChain(\n",
        "                    llm=lc_llmv4,\n",
        "                    prompt=prompt,\n",
        "                    output_parser=StrOutputParser())\n",
        "    context = ''.join(map(str, topk_context))\n",
        "    context\n",
        "    return chain.run(context = context, question = query)\n",
        "\n",
        "tools = [conversational_retrieval_chain_QA, LLM_chain_QA]\n",
        "# llm_with_tools = lc_llmv4.bind_tools(tools)\n",
        "\n",
        "# from langchain import hub\n",
        "# prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
        "# prompt.pretty_print()\n",
        "# import langchain_core\n",
        "# import typing\n",
        "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate,MessagesPlaceholder,HumanMessagePromptTemplate\n",
        "\n",
        "# prompt = ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'],\n",
        "#                             input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage,\n",
        "#                                                                                   langchain_core.messages.human.HumanMessage,\n",
        "#                                                                                   langchain_core.messages.chat.ChatMessage,\n",
        "#                                                                                   langchain_core.messages.system.SystemMessage,\n",
        "#                                                                                   langchain_core.messages.function.FunctionMessage,\n",
        "#                                                                                   langchain_core.messages.tool.ToolMessage]],\n",
        "#                                                                                   'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage,\n",
        "#                                                                                                                                langchain_core.messages.human.HumanMessage,\n",
        "#                                                                                                                                langchain_core.messages.chat.ChatMessage,\n",
        "#                                                                                                                                langchain_core.messages.system.SystemMessage,\n",
        "#                                                                                                                                langchain_core.messages.function.FunctionMessage,\n",
        "#                                                                                                                                langchain_core.messages.tool.ToolMessage]]},\n",
        "#                                                                                                                                metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-tools-agent', 'lc_hub_commit_hash': 'c18672812789a3b9697656dd539edf0120285dcae36396d0b548ae42a4ed66f5'},\n",
        "#                                                                                                                                messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),\n",
        "#                                                                                                                                          MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
        "#                                                                                                                                          HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),\n",
        "#                                                                                                                                          MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
        "\n",
        "\n",
        "from langchain.tools.render import render_text_description\n",
        "\n",
        "rendered_tools = render_text_description(tools)\n",
        "rendered_tools\n",
        "\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_prompt = f\"\"\"You are an assistant that has access to the following set of tools. Here are the names and descriptions for each tool:\n",
        "\n",
        "{rendered_tools}\n",
        "\n",
        "Given the user input, return the name and input of the tool to use. Return your response as a JSON blob with 'name' and 'arguments' keys.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
        ")\n",
        "\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "# model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "chain = prompt | lc_llmv4 | JsonOutputParser()\n",
        "chain.invoke({\"input\": \"Summarize the document as per conversational retrieval\"})\n",
        "\n",
        "retriever.get_relevant_documents(\"summarize the content\")"
      ],
      "metadata": {
        "id": "zY5THMM9TGb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "from llama_index.core.prompts import LangchainPromptTemplate\n",
        "\n",
        "lc_prompt_tmpl = LangchainPromptTemplate(\n",
        "    template=PROMPT,\n",
        "    template_var_mappings={\"query_str\": \"question\", \"context_str\": \"context\"},\n",
        ")\n",
        "\n",
        "query_engine.update_prompts(\n",
        "    {\"response_synthesizer:text_qa_template\": lc_prompt_tmpl}\n",
        ")\n",
        "prompts_dict = query_engine.get_prompts()\n",
        "\n",
        "# define prompt viewing function\n",
        "def display_prompt_dict(prompts_dict):\n",
        "    for k, p in prompts_dict.items():\n",
        "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
        "        display(Markdown(text_md))\n",
        "        print(p.get_template())\n",
        "        display(Markdown(\"<br><br>\"))\n",
        "prompts_dict = query_engine.get_prompts()\n",
        "# display_prompt_dict(prompts_dict)\n",
        "\n",
        "display_prompt_dict(prompts_dict)\n",
        "\n",
        "\n",
        "\n",
        "def apim_token_response():\n",
        "\n",
        "    apim_response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "    return apim_response\n",
        "\n",
        "def web_reponses(query:str):\n",
        "    apim_response = apim_token_response()\n",
        "\n",
        "    headers = {\n",
        "    'apiVersion': '1',\n",
        "    'Content-Type': 'application/json',\n",
        "    'Authorization': 'Bearer '+apim_response.json()['access_token']\n",
        "    }\n",
        "    web_response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "    return web_response\n",
        "\n",
        "def web_prompt_tool(query):\n",
        "    web_response = web_reponses(query)\n",
        "    print(web_response)\n",
        "    snippets = []\n",
        "    for i in range(0,len(web_response.json()['result']['webPages']['value'])):\n",
        "        val = web_response.json()['result']['webPages']['value'][i]\n",
        "        snippets.append(val['snippet'])\n",
        "    return \" \".join(snippets)\n",
        "    # return query+\" web tools\"\n",
        "web_prompt_tool(\"Open source LLM models\")\n",
        "\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.indexes import SearchIndexClient\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "from llama_index.core.settings import Settings\n",
        "\n",
        "from llama_index.llms.azure_openai import AzureOpenAI\n",
        "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
        "from llama_index.vector_stores.azureaisearch import AzureAISearchVectorStore\n",
        "from llama_index.vector_stores.azureaisearch import (\n",
        "    IndexManagement,\n",
        "    MetadataIndexFieldType,\n",
        ")\n",
        "\n",
        "\n",
        "from llama_index.agent.openai import OpenAIAgent\n",
        "# from llama_index.core.tools.google import GmailToolSpec\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def normal_regular_prompt(query):\n",
        "    return llm.complete(query)\n",
        "from llama_index.core.agent import ReActAgent\n",
        "website_prompt_tool = FunctionTool.from_defaults(fn=web_prompt_tool)\n",
        "normal_regular_prompt_tool = FunctionTool.from_defaults(fn=normal_regular_prompt)\n",
        "# tools = [function_tool1,function_tool2]\n",
        "\n",
        "\n",
        "import json\n",
        "from typing import Sequence, List\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.tools import BaseTool, FunctionTool\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "def doc_qa(query) -> str:\n",
        "    # return index.as_query_engine(similarity_top_k=10).query(query).response\n",
        "    return query_engine.query(query).response\n",
        "\n",
        "doc_qa_tool = FunctionTool.from_defaults(fn=doc_qa)\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "tools = [doc_qa_tool,\n",
        "        #  multiply_tool,\n",
        "        #  add_tool,\n",
        "         website_prompt_tool,\n",
        "         normal_regular_prompt_tool\n",
        "         ]\n",
        "\n",
        "\n",
        "from llama_index.core.agent import AgentRunner\n",
        "from llama_index.agent.openai import OpenAIAgentWorker, OpenAIAgent\n",
        "\n",
        "from llama_index.core.agent import AgentRunner, ReActAgentWorker, ReActAgent\n",
        "# Option 1: Initialize OpenAIAgent\n",
        "agent = ReActAgent.from_tools(tools, llm=llm, verbose=True)\n",
        "# agent.chat(\"what are the sections mentioned in the document?\")\n",
        "tools[0]._metadata"
      ],
      "metadata": {
        "id": "tE0GXyY_TGZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
        "tools = [\n",
        "    # Tool(\n",
        "    #     name=\"Loading dialect\",\n",
        "    #     func=load_dialect,\n",
        "    #     description=\"useful for when loading database dialect\",\n",
        "    # ),\n",
        "    # Tool(\n",
        "    #     name = \"Loading the table prefix\",\n",
        "    #     func=load_table_prefix,\n",
        "    #     description=\"useful for when loading table prefix\"\n",
        "    # ),\n",
        "    # Tool(\n",
        "    #     name=\"Loading the listed tables\",\n",
        "    #     func=load_listed_tables,\n",
        "    #     description=\"useful for when loading listed tables\"\n",
        "    # ),\n",
        "    Tool(\n",
        "        name=\"fetching the few shot examples\",\n",
        "        func=semantic_examples,\n",
        "        description=\"useful for when fetching the few shot examples\"\n",
        "    )\n",
        "]\n",
        "\n",
        "from langchain import LLMMathChain\n",
        "from langchain.agents import AgentType, initialize_agent\n",
        "from langchain.agents import (\n",
        "    Tool,\n",
        "    AgentExecutor,\n",
        "    LLMSingleActionAgent,\n",
        "    AgentOutputParser,\n",
        ")\n",
        "import re\n",
        "from langchain.llms import AzureOpenAI\n",
        "import json\n",
        "\n",
        "from langchain.tools import BaseTool, StructuredTool, Tool, tool\n",
        "from langchain.prompts import PromptTemplate, StringPromptTemplate\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "from typing import List, Union\n",
        "from pydantic import BaseModel, Field\n",
        "# from rdflib import Graph\n",
        "# from rapidfuzz import fuzz\n",
        "from langchain.chains import LLMChain\n",
        "# from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "\n",
        "# Set up a prompt template\n",
        "class CustomPromptTemplate(StringPromptTemplate):\n",
        "    # The template to use\n",
        "    template: str\n",
        "    # The list of tools available\n",
        "    tools: List[Tool]\n",
        "\n",
        "    def format(self, **kwargs) -> str:\n",
        "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
        "        # Format them in a particular way\n",
        "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
        "        # print(intermediate_steps)\n",
        "        thoughts = \"\"\n",
        "        for action, observation in intermediate_steps:\n",
        "            thoughts += action.log\n",
        "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
        "        # Set the agent_scratchpad variable to that value\n",
        "        kwargs[\"agent_scratchpad\"] = thoughts\n",
        "        # Create a tools variable from the list of tools provided\n",
        "        kwargs[\"tools\"] = \"\\n\".join(\n",
        "            [f\"{tool.name}: {tool.description}\" for tool in self.tools]\n",
        "        )\n",
        "        # Create a list of tool names for the tools provided\n",
        "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
        "        return self.template.format(**kwargs)\n",
        "\n",
        "\n",
        "prompt = CustomPromptTemplate(\n",
        "    input_variables=[\"input\", \"intermediate_steps\"], tools=tools, template=template\n",
        ")\n",
        "\n",
        "\n",
        "class CustomOutputParser(AgentOutputParser):\n",
        "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "        # Check if agent should finish\n",
        "        if \"Final Answer:\" in llm_output:\n",
        "            return AgentFinish(\n",
        "                # Return values is generally always a dictionary with a single `output` key\n",
        "                # It is not recommended to try anything else at the moment :)\n",
        "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
        "                log=llm_output,\n",
        "            )\n",
        "        # Parse out the action and action input\n",
        "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
        "        match = re.search(regex, llm_output, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
        "        action = match.group(1).strip()\n",
        "        action_input = match.group(2)\n",
        "        # Return the action and action input\n",
        "        return AgentAction(\n",
        "            tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output\n",
        "        )\n",
        "output_parser = CustomOutputParser()\n",
        "chain = LLMChain(llm=lc_llmv4, prompt=prompt)\n",
        "tool_names = [tool.name for tool in tools]\n",
        "agent = LLMSingleActionAgent(\n",
        "    llm_chain=chain,\n",
        "    output_parser=output_parser,\n",
        "    stop=[\"\\nObservation:\"],\n",
        "    allowed_tools=tool_names,\n",
        ")\n",
        "\n",
        "agent_exec = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "agent_exec(\"List the tables\")\n",
        "\n",
        "\n",
        "from langchain.agents import AgentExecutor, Tool, ZeroShotAgent\n",
        "prompt = ZeroShotAgent.create_prompt(\n",
        "    tools,\n",
        "    prefix=template_1,\n",
        "    suffix=template6,\n",
        "    input_variables=[\"input\",\n",
        "                    #  \"chat_history\",\n",
        "                     \"agent_scratchpad\"\n",
        "                     ],\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(llm=lc_llmv4, prompt=prompt)\n",
        "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\n",
        "agent_chain = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent, tools=tools, verbose=True,\n",
        "    # memory=memory,\n",
        "    return_intermediate_steps=True,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "W_7jfGAOTGWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import Sequence, List\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.tools import BaseTool, FunctionTool\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "tools = [multiply_tool, add_tool]\n",
        "\n",
        "\n",
        "from llama_index.core.agent import AgentRunner\n",
        "from llama_index.agent.openai import OpenAIAgentWorker, OpenAIAgent\n",
        "\n",
        "# Option 1: Initialize OpenAIAgent\n",
        "# agent = OpenAIAgent.from_tools(tools, llm=llm, verbose=True)\n",
        "\n",
        "# openai_step_engine = OpenAIAgentWorker.from_tools(tools, llm=llm, verbose=True)\n",
        "# agent = AgentRunner(openai_step_engine)\n",
        "\n",
        "from llama_index.core.agent import AgentRunner, ReActAgentWorker, ReActAgent\n",
        "# Option 1: Initialize OpenAIAgent\n",
        "agent = ReActAgent.from_tools(tools, llm=llm, verbose=True)\n",
        "agent.chat(\"Hi\")\n",
        "\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "from llama_index.core.prompts import LangchainPromptTemplate\n",
        "\n",
        "lc_prompt_tmpl = LangchainPromptTemplate(\n",
        "    template=PROMPT,\n",
        "    template_var_mappings={\"query_str\": \"question\", \"context_str\": \"context\"},\n",
        ")\n",
        "\n",
        "query_engine.update_prompts(\n",
        "    {\"response_synthesizer:text_qa_template\": lc_prompt_tmpl}\n",
        ")\n",
        "prompts_dict = query_engine.get_prompts()\n",
        "\n",
        "# define prompt viewing function\n",
        "def display_prompt_dict(prompts_dict):\n",
        "    for k, p in prompts_dict.items():\n",
        "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
        "        display(Markdown(text_md))\n",
        "        print(p.get_template())\n",
        "        display(Markdown(\"<br><br>\"))\n",
        "prompts_dict = query_engine.get_prompts()\n",
        "# display_prompt_dict(prompts_dict)\n",
        "\n",
        "display_prompt_dict(prompts_dict)\n",
        "\n",
        "index.as_retriever(similarity_top_k=10).retrieve(\"what are the sections mentioned in the document?\")\n",
        "\n",
        "\n",
        "def web_prompt_tool(query):\n",
        "    web_response = web_reponses(query)\n",
        "    print(web_response)\n",
        "    snippets = []\n",
        "    for i in range(0,len(web_response.json()['result']['webPages']['value'])):\n",
        "        val = web_response.json()['result']['webPages']['value'][i]\n",
        "        snippets.append(val['snippet'])\n",
        "    return \" \".join(snippets)\n",
        "    # return query+\" web tools\"\n",
        "web_prompt_tool(\"Open source LLM models\")\n",
        "\n",
        "\n",
        "import logging\n",
        "import sys\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "from azure.search.documents import SearchClient\n",
        "from azure.search.documents.indexes import SearchIndexClient\n",
        "from IPython.display import Markdown, display\n",
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    StorageContext,\n",
        "    VectorStoreIndex,\n",
        ")\n",
        "from llama_index.core.settings import Settings\n",
        "\n",
        "from llama_index.llms.azure_openai import AzureOpenAI\n",
        "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
        "from llama_index.vector_stores.azureaisearch import AzureAISearchVectorStore\n",
        "from llama_index.vector_stores.azureaisearch import (\n",
        "    IndexManagement,\n",
        "    MetadataIndexFieldType,\n",
        ")\n",
        "\n",
        "\n",
        "from llama_index.agent.openai import OpenAIAgent\n",
        "# from llama_index.core.tools.google import GmailToolSpec\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def normal_regular_prompt(query):\n",
        "    return llm.complete(query)\n",
        "from llama_index.core.agent import ReActAgent\n",
        "website_prompt_tool = FunctionTool.from_defaults(fn=web_prompt_tool)\n",
        "normal_regular_prompt_tool = FunctionTool.from_defaults(fn=normal_regular_prompt)\n",
        "# tools = [function_tool1,function_tool2]\n",
        "\n",
        "\n",
        "import json\n",
        "from typing import Sequence, List\n",
        "\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.tools import BaseTool, FunctionTool\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "def doc_qa(query) -> str:\n",
        "    # return index.as_query_engine(similarity_top_k=10).query(query).response\n",
        "    return query_engine.query(query).response\n",
        "\n",
        "doc_qa_tool = FunctionTool.from_defaults(fn=doc_qa)\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "\n",
        "tools = [doc_qa_tool,multiply_tool,add_tool,website_prompt_tool,normal_regular_prompt_tool]\n",
        "\n",
        "\n",
        "from llama_index.core.agent import AgentRunner\n",
        "from llama_index.agent.openai import OpenAIAgentWorker, OpenAIAgent\n",
        "\n",
        "from llama_index.core.agent import AgentRunner, ReActAgentWorker, ReActAgent\n",
        "# Option 1: Initialize OpenAIAgent\n",
        "agent = ReActAgent.from_tools(tools, llm=llm, verbose=True)\n",
        "# agent.chat(\"what are the sections mentioned in the document?\")\n",
        "tools[0]._metadata\n",
        "\n",
        "\n",
        "task = agent.create_task(\"what are the sections mentioned in the document?\")\n",
        "step_output = agent.run_step(task.task_id)\n",
        "step_output.output"
      ],
      "metadata": {
        "id": "KcGTob7BVPbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_hist_prompt = '''\n",
        "            {chat_history}\n",
        "            Question: {question}\n",
        "            Answer:\n",
        "            '''\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "prefix_prompt_template = prefix_prompt_template.format(context = context,\n",
        "                )\n",
        "prompt_template = prefix_prompt_template + chat_hist_prompt\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"question\"], template=prompt_template\n",
        "    )\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "lc_llmv4 = GPTV4()\n",
        "lc_llmv35 = GPTV35()\n",
        "lc_llm = lc_llmv4.with_fallbacks([lc_llmv35])\n",
        "logger.info(\"lodaed LLM\" + str(lc_llm))\n",
        "chat_llm_chain = LLMChain(\n",
        "    llm=lc_llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        "    )\n",
        "prompt_template\n",
        "\n",
        "chat_llm_chain.predict(question=query)\n",
        "\n",
        "\n",
        "from langchain.tools import StructuredTool\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.agents import Tool\n",
        "\n",
        "def doc_prompt_tool(lc_llm):\n",
        "    def doc_prompt_answering_fn(query:str):\n",
        "        chat_llm_chain.predict(question=query)\n",
        "    doc_prompt_answering = StructuredTool.from_function(\n",
        "        func=doc_prompt_answering_fn,\n",
        "        name=\"Document Prompt Answering\",\n",
        "        description=\"Useful when the user is asking the queries/questions from the uploaded document\",\n",
        "    )\n",
        "    return doc_prompt_answering\n",
        "\n",
        "\n",
        "doc_prompt_answering_tool = doc_prompt_tool(lc_llm)\n",
        "\n",
        "tools = [\n",
        "        Tool(\n",
        "            name=\"Document Prompt Answering\",\n",
        "            func=doc_prompt_answering_tool.run,\n",
        "            description=\"Useful when the user is asking the queries/questions from the uploaded document\",\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "from langchain.agents import AgentType, Tool, initialize_agent\n",
        "agent = initialize_agent(\n",
        "            tools, lc_llm,\n",
        "            agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "            verbose=False,\n",
        "            max_iterations=5,\n",
        "            return_intermediate_steps=True,\n",
        "            handle_parsing_errors=True,\n",
        "        )\n",
        "\n",
        "response = agent({\"input\": \"summarize the uploaded document\"})\n",
        "\n",
        "\n",
        "from langchain.chat_models import AzureChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "chain = ConversationalRetrievalChain.from_llm(\n",
        "        lc_llm,\n",
        "        retriever,\n",
        "        return_source_documents=True,\n",
        "        combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
        "    )\n",
        "\n",
        "\n",
        "result = chain({\"question\": query,\n",
        "                \"chat_history\":[],},return_only_outputs=False,)\n",
        "\n",
        "from langchain.tools import StructuredTool\n",
        "from langchain.schema import HumanMessage\n",
        "from langchain.agents import Tool\n",
        "\n",
        "def doc_prompt_tool(lc_llm):\n",
        "    def doc_prompt_answering_fn(query:str):\n",
        "        # chat_llm_chain.predict(question=query)\n",
        "        chain.run({\"question\":query,\"chat_history\":[],})\n",
        "    doc_prompt_answering = StructuredTool.from_function(\n",
        "        func=doc_prompt_answering_fn,\n",
        "        name=\"Document Prompt Answering\",\n",
        "        description=\"Useful when the user is asking the queries/questions from the uploaded document\",\n",
        "    )\n",
        "    return doc_prompt_answering\n",
        "\n",
        "\n",
        "doc_prompt_answering_tool = doc_prompt_tool(lc_llm)\n",
        "\n",
        "tools = [\n",
        "        Tool(\n",
        "            name=\"Document Prompt Answering\",\n",
        "            func=doc_prompt_answering_tool.run,\n",
        "            description=\"Useful when the user is asking the queries/questions from the uploaded document\",\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "conversational_memory = ConversationBufferWindowMemory(\n",
        "    memory_key='chat_history',\n",
        "    k=5,\n",
        "    return_messages=True\n",
        ")\n",
        "agent = initialize_agent(\n",
        "    # agent='chat-conversational-react-description',\n",
        "    agent='chat-zero-shot-react-description',\n",
        "    tools=tools,\n",
        "    llm=lc_llm,\n",
        "    verbose=True,\n",
        "    # max_iterations=3,\n",
        "    # early_stopping_method='generate',\n",
        "    # memory=conversational_memory,\n",
        "    # verbose=False,\n",
        "    max_iterations=5,\n",
        "    return_intermediate_steps=True,\n",
        "    handle_parsing_errors=False,\n",
        ")\n",
        "\n",
        "result = agent.invoke({\"input\":\"list of coverages mentioned in the document\"})\n",
        "\n",
        "\n",
        "from langchain.agents import AgentExecutor, Tool, create_react_agent\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "# prompt = PromptTemplate.from_template(PROMPT)\n",
        "\n",
        "# # llm = lc_llm\n",
        "# chat_model_with_stop = llm.bind(stop=[\"Invalid or incomplete response\"])\n",
        "# agent = create_react_agent(chat_model_with_stop, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent,\n",
        "                               tools=tools,\n",
        "                               verbose=True,\n",
        "                                max_iterations=5,\n",
        "                                # return_intermediate_steps=True,\n",
        "                                handle_parsing_errors=True,)\n",
        "cfg = RunnableConfig()\n",
        "\n",
        "\n",
        "answer1 = agent_executor.invoke({\"input\": query}, cfg)\n",
        "answer1"
      ],
      "metadata": {
        "id": "wNGHy1moVPYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, Tool, create_react_agent\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "prompt = PromptTemplate.from_template(PROMPT)\n",
        "\n",
        "# llm = lc_llm\n",
        "chat_model_with_stop = llm.bind(stop=[\"Invalid or incomplete response\"])\n",
        "agent = create_react_agent(chat_model_with_stop, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent,\n",
        "                               tools=tools,\n",
        "                               verbose=True,\n",
        "                                max_iterations=5,\n",
        "                                return_intermediate_steps=True,\n",
        "                                handle_parsing_errors=True,)\n",
        "cfg = RunnableConfig()\n",
        "agent_executor.invoke({\"input\": \"List of open source token classification models from huggingface\"}, cfg)\n",
        "answer1['intermediate_steps'][-1][0].log.split(\"Final Answer:\")[-1]"
      ],
      "metadata": {
        "id": "k_TAv9l2VPVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_hist_prompt = '''\n",
        "\n",
        "            {chat_history}\n",
        "            Question: {human_input}\\n\n",
        "            SQLQuery:\n",
        "            '''\n",
        "\n",
        "template = prefix_template + suffix_template\n",
        "template = template.format(dialect = db.dialect,\n",
        "                table_prefix=TABLE_PREFIX,\n",
        "                schema=schema,\n",
        "                )\n",
        "full_prompt = template + chat_hist_prompt\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"human_input\"], template=full_prompt\n",
        "    )\n",
        "\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "chat_llm_chain = LLMChain(\n",
        "llm=lc_llm,\n",
        "prompt=prompt,\n",
        "verbose=True,\n",
        "memory=memory,\n",
        ")\n",
        "\n",
        "chat_llm_chain.predict(human_input=\"List all items\")\n",
        "sql_query"
      ],
      "metadata": {
        "id": "L4sh-WpWVPTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dspy.settings.configure(lm=llm, rm=azure_search_retriever)\n",
        "sentence = \"disney again ransacks its archives for a quick-buck sequel .\"  # example from the SST-2 dataset.\n",
        "\n",
        "classify = dspy.Predict('sentence -> sentiment')\n",
        "print(classify(sentence=sentence).sentiment)"
      ],
      "metadata": {
        "id": "rcBQgQKeXgAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mat_doc = documents\n",
        "\n",
        "topk_context = []\n",
        "\n",
        "for i in range(0,len(documents)):\n",
        "    topk_context.append(\"Source File : \"+str(mat_doc[i].metadata['source'].split(\"\\\\\")[-1])+\", Page Number :\"+str(mat_doc[i].metadata['page'])+\", Content:\"+mat_doc[i].page_content)\n",
        "\n",
        "from haystack import Document\n",
        "\n",
        "documents = []\n",
        "\n",
        "for i in range(0,len(topk_context)):\n",
        "    documents.append(Document(content=topk_context[i]))\n",
        "\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\n",
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "import os\n",
        "# doc_embedder = SentenceTransformersDocumentEmbedder(model=model_name)\n",
        "\n",
        "from haystack.components.embedders import AzureOpenAITextEmbedder, AzureOpenAIDocumentEmbedder\n",
        "\n",
        "docs_with_embeddings = doc_embedder.run(documents)[\"documents\"]\n",
        "\n",
        "doc_store = InMemoryDocumentStore()\n",
        "doc_store.write_documents(docs_with_embeddings)\n",
        "retriever = InMemoryEmbeddingRetriever(doc_store,top_k=top_k)\n",
        "\n",
        "# text_embedder = SentenceTransformersTextEmbedder(model=model_name)\n",
        "text_embedder = AzureOpenAITextEmbedder(azure_endpoint=\"\",\n",
        "                                                                        # azure_deployment=\"text-embedding-ada-002\",\n",
        "                                                                        azure_deployment= \"openai_text_embedding_ada_002\",)\n",
        "# Not appl for Azure Embedding\n",
        "# text_embedder.warm_up()\n",
        "query_embedding = text_embedder.run(query)[\"embedding\"]\n",
        "\n",
        "result = retriever.run(query_embedding=query_embedding)\n",
        "\n",
        "print(result[\"documents\"])\n",
        "\n",
        "ctx = []\n",
        "for i in range(0,len(result[\"documents\"])):\n",
        "    ctx.append(result[\"documents\"][i].content)\n",
        "ctx = '\\n'.join(map(str, ctx))\n",
        "\n",
        "suffix_template = \"\"\"Question: {{ question }}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "final_template = prefix+\"\\n\"+ctx+\"\\n\"+suffix_template\n",
        "final_template\n",
        "\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.generators import AzureOpenAIGenerator\n",
        "import os\n",
        "template = final_template\n",
        "prompt_builder = PromptBuilder(template=template)\n",
        "\n",
        "from haystack import Pipeline\n",
        "pipe = Pipeline()\n",
        "pipe.add_component(\"prompt_builder\", PromptBuilder(template=template))\n",
        "pipe.add_component(\"llm\", AzureOpenAIGenerator(azure_endpoint=\"\", azure_deployment=\"openai_gpt_4_32k\"))\n",
        "pipe.connect(\"prompt_builder\", \"llm\")\n",
        "\n",
        "result = pipe.run({\n",
        "    \"prompt_builder\": {\n",
        "        \"question\": query\n",
        "    }\n",
        "})\n",
        "result['llm']['replies']\n",
        "\n",
        "\n",
        "from haystack import Pipeline\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "from haystack.components.embedders import AzureOpenAITextEmbedder, AzureOpenAIDocumentEmbedder\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
        "\n",
        "document_store = InMemoryDocumentStore(embedding_similarity_function=\"cosine\")\n",
        "\n",
        "documents = [Document(content=\"My name is Wolfgang and I live in Berlin\"),\n",
        "             Document(content=\"I saw a black horse running\"),\n",
        "             Document(content=\"Germany has many big cities\")]\n",
        "\n",
        "indexing_pipeline = Pipeline()\n",
        "indexing_pipeline.add_component(\"embedder\", AzureOpenAIDocumentEmbedder())\n",
        "indexing_pipeline.add_component(\"writer\", DocumentWriter(document_store=document_store))\n",
        "indexing_pipeline.connect(\"embedder\", \"writer\")\n",
        "\n",
        "indexing_pipeline.run({\"embedder\": {\"documents\": documents}})\n",
        "\n",
        "query_pipeline = Pipeline()\n",
        "query_pipeline.add_component(\"text_embedder\", AzureOpenAITextEmbedder())\n",
        "query_pipeline.add_component(\"retriever\", InMemoryEmbeddingRetriever(document_store=document_store))\n",
        "query_pipeline.connect(\"text_embedder.embedding\", \"retriever.query_embedding\")\n",
        "\n",
        "query = \"Who lives in Berlin?\"\n",
        "\n",
        "result = query_pipeline.run({\"text_embedder\":{\"text\": query}})\n",
        "\n",
        "print(result['retriever']['documents'][0])"
      ],
      "metadata": {
        "id": "7gZzGe7hXf8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_sql_query_chain\n",
        "database_chain = create_sql_query_chain(lc_llm,db)\n",
        "database_chain.get_prompts()[0].input_variables = [\n",
        "                                                #    'dialect',\n",
        "                                                #    'few_shot_examples',\n",
        "                                                #    'table_prefix',\n",
        "                                                #    'table_info',\n",
        "                                                   'input'\n",
        "                                                   ]\n",
        "database_chain.get_prompts()[0].partial_variables = {'dialect': 'snowflake',\n",
        "                                                     \"few_shot_examples\":few_shot_template,\n",
        "                                                     \"table_prefix\":TABLE_PREFIX,\n",
        "                                                     \"table_info\":db.table_info,}\n",
        "database_chain.get_prompts()[0].template = template\n",
        "\n",
        "database_chain.get_prompts()[0].pretty_print()\n",
        "\n",
        "database_chain.invoke({\n",
        "                        # \"dialect\": \"snowflake\",\n",
        "                     #   \"few_shot_examples\":few_shot_template,\n",
        "                     #   \"table_prefix\":TABLE_PREFIX,\n",
        "                     #   \"table_info\":db.table_info,\n",
        "                    #    \"query_str\":\"list down all tables\",\n",
        "                       \"question\":\"list down all tables\",})\n",
        "\n",
        "query_engine = NLSQLTableQueryEngine(\n",
        "    sql_database=db, tables=include_tables,\n",
        "    llm=llm\n",
        ")\n",
        "schema = db.table_info\n",
        "logger.info(\"loaded variables\")\n",
        "\n",
        "def return_db_prefix(**kwargs):\n",
        "    return \"RAW_DMP_SB_DEV_DMP\"\n",
        "\n",
        "def return_db_info(**kwargs):\n",
        "    return str(db.table_info)\n",
        "\n",
        "def return_db_diaelect(**kwargs):\n",
        "    return str(db.dialect)\n",
        "\n",
        "def semantic_examples(**kwargs):\n",
        "    query_str = kwargs[\"query_str\"]\n",
        "    corpus = []\n",
        "    for i in range(0,len(examples)):\n",
        "        corpus.append(examples[i]['input'])\n",
        "    embedder = SentenceTransformer(r\"C:\\Cacasi\\cacasci\\src\\policy_enhancements\\all-MiniLM-L6-v2\")\n",
        "    corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
        "    queries = [query_str]\n",
        "    example_match = []\n",
        "    for query in queries:\n",
        "        query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "        hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
        "        hits = hits[0]\n",
        "        for hit in hits:\n",
        "            inner_dict = {}\n",
        "            inner_dict[\"input\"] = corpus[hit['corpus_id']]\n",
        "            for i in range(0,len(examples)):\n",
        "                if corpus[hit['corpus_id']] in examples[i]['input']:\n",
        "                    inner_dict[\"query\"] = examples[i]['query']\n",
        "            example_match.append(inner_dict)\n",
        "    return '\\n'.join(map(str, example_match)).replace(\"{\",\"\").replace(\"}\",\"\")\n",
        "\n",
        "template = prefix_template + suffix_template\n",
        "txt_sql_prompt_tmpl = PromptTemplate(template,\n",
        "                                    function_mappings={\"few_shot_examples\": semantic_examples,\n",
        "                                                    \"table_prefix\": return_db_prefix,\n",
        "                                                    \"dialect\": return_db_diaelect,\n",
        "                                                    \"schema\": return_db_info}\n",
        "                                    )\n",
        "\n",
        "query_engine.update_prompts(\n",
        "    {\"sql_retriever:text_to_sql_prompt\": txt_sql_prompt_tmpl}\n",
        ")\n",
        "logger.info(\"prompt****\"+str(query_engine.get_prompts()))\n",
        "\n",
        "# define prompt viewing function\n",
        "from IPython.display import Markdown, display\n",
        "from pprint import pprint\n",
        "def display_prompt_dict(prompts_dict):\n",
        "    for k, p in prompts_dict.items():\n",
        "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
        "        display(Markdown(text_md))\n",
        "        pprint(p.get_template())\n",
        "        display(Markdown(\"<br><br>\"))\n",
        "prompts_dict = query_engine.get_prompts()\n",
        "display_prompt_dict(prompts_dict)\n",
        "\n",
        "query_str = \"list down all tables\"\n",
        "response = query_engine.query(query_str)\n",
        "sql_query = response.metadata['sql_query']\n",
        "\n",
        "from llama_index.core import Document\n",
        "documents = [Document(\n",
        "    text=\"The SQL query generation has been asked as:\"+query_str+\"\\n\"+\"The SQL query generated is :\"+sql_query+\"\\n\"+\"Executed SQL Result:\"+pd.DataFrame(db._execute(sql_query)).to_markdown(),\n",
        "    metadata={\"filename\": \"local\", \"category\": \"summarization\"},\n",
        ")]\n",
        "from llama_index.core import VectorStoreIndex\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "index.as_query_engine(similarity_top_k=10).query(\"Can you please summarize the content of document that has SQL Query & its table results?\")\n",
        "\n",
        "\n",
        "template = prefix_template + suffix_template\n",
        "txt_sql_prompt_tmpl = PromptTemplate(template,\n",
        "                                    function_mappings={\"few_shot_examples\": semantic_examples,\n",
        "                                                    \"table_prefix\": return_db_prefix}\n",
        "                                    )\n",
        "full_prompt = txt_sql_prompt_tmpl.format(\n",
        "    query_str=\"list down all tables\",dialect = db.dialect,schema = schema\n",
        ")\n",
        "logger.info(\"full_prompt****\"+str(full_prompt))\n",
        "sql_query = llm.complete(full_prompt).text\n",
        "logger.info(\"sql_query****\"+str(sql_query))\n",
        "\n",
        "# define prompt viewing function\n",
        "from IPython.display import Markdown, display\n",
        "from pprint import pprint\n",
        "def display_prompt_dict(prompts_dict):\n",
        "    for k, p in prompts_dict.items():\n",
        "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
        "        display(Markdown(text_md))\n",
        "        pprint(p.get_template())\n",
        "        display(Markdown(\"<br><br>\"))\n",
        "prompts_dict = query_engine.get_prompts()\n",
        "display_prompt_dict(prompts_dict)\n",
        "\n",
        "query_engine.update_prompts(\n",
        "    {\"sql_retriever:text_to_sql_prompt\": txt_sql_prompt_tmpl}\n",
        ")\n",
        "\n",
        "prompts_dict = query_engine.get_prompts()\n",
        "display_prompt_dict(prompts_dict)\n",
        "\n",
        "query_str = \"list down all tables\"\n",
        "logger.info(\"query_str****\"+str(query_str))\n",
        "example_match = semantic_examples(query_str)\n",
        "few_shot_template = '\\n'.join(map(str, example_match)).replace(\"{\",\"\").replace(\"}\",\"\")\n",
        "\n",
        "template = left_template + few_shot_template + right_template\n",
        "txt_sql_prompt_tmpl = PromptTemplate(template)\n",
        "\n",
        "query_engine.update_prompts(\n",
        "    {\"sql_retriever:text_to_sql_prompt\": txt_sql_prompt_tmpl}\n",
        ")\n",
        "response = query_engine.query(query_str)\n",
        "sql_query = response.metadata['sql_query']\n",
        "sql_query\n",
        "\n",
        "logger.info(\"****query_str****\"+str(query_str))\n",
        "example_match = semantic_examples(query_str)\n",
        "few_shot_template = '\\n'.join(map(str, example_match)).replace(\"{\",\"\").replace(\"}\",\"\")\n",
        "\n",
        "template = left_template + few_shot_template + right_template\n",
        "txt_sql_prompt_tmpl = PromptTemplate(template)\n",
        "\n",
        "full_prompt = txt_sql_prompt_tmpl.format(\n",
        "    query_str=query_str,dialect = db.dialect,schema = schema\n",
        ")\n",
        "logger.info(\"****full_prompt****\"+str(full_prompt))\n",
        "sql_query = llm.complete(full_prompt).text\n",
        "logger.info(\"****sql_query****\"+str(sql_query))\n",
        "\n",
        "\n",
        "query_engine = NLSQLTableQueryEngine(\n",
        "    sql_database=db, tables=include_tables,\n",
        "    llm=llm\n",
        ")\n",
        "schema = db.table_info\n",
        "logger.info(\"loaded variables\")\n",
        "\n",
        "def semantic_examples(query_str):\n",
        "    corpus = []\n",
        "    for i in range(0,len(examples)):\n",
        "        corpus.append(examples[i]['input'])\n",
        "    embedder = SentenceTransformer(r\"C:\\Cacasi\\cacasci\\src\\policy_enhancements\\all-MiniLM-L6-v2\")\n",
        "    corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
        "    queries = [query_str]\n",
        "    example_match = []\n",
        "    for query in queries:\n",
        "        query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "        hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
        "        hits = hits[0]\n",
        "        for hit in hits:\n",
        "            inner_dict = {}\n",
        "            inner_dict[\"input\"] = corpus[hit['corpus_id']]\n",
        "            for i in range(0,len(examples)):\n",
        "                if corpus[hit['corpus_id']] in examples[i]['input']:\n",
        "                    inner_dict[\"query\"] = examples[i]['query']\n",
        "            example_match.append(inner_dict)\n",
        "    return example_match\n",
        "\n",
        "def return_response(query_str):\n",
        "    logger.info(\"query_str****\"+str(query_str))\n",
        "    example_match = semantic_examples(query_str)\n",
        "    few_shot_template = '\\n'.join(map(str, example_match)).replace(\"{\",\"\").replace(\"}\",\"\")\n",
        "\n",
        "    template = left_template + few_shot_template + right_template\n",
        "    txt_sql_prompt_tmpl = PromptTemplate(template)\n",
        "\n",
        "    query_engine.update_prompts(\n",
        "        {\"sql_retriever:text_to_sql_prompt\": txt_sql_prompt_tmpl}\n",
        "    )\n",
        "    response = query_engine.query(query_str)\n",
        "    sql_query = response.metadata['sql_query']\n",
        "    try:\n",
        "        response = \"SQL Query:\"+sql_query+\"\\n\"+\"Execution Result:\"+\"\\n\"+str(pd.DataFrame(db._execute(sql_query)).to_markdown())\n",
        "    except:\n",
        "        response = \"SQL Query:\"+sql_query+\"\\n\"+\"Execution Result:\"+\"\\n\"+str(\"Unable to execute the query due to exception\")\n",
        "    logger.info(\"response***\"+str(response))\n",
        "    return response\n",
        "\n",
        "def return_response1(query_str):\n",
        "    logger.info(\"****query_str****\"+str(query_str))\n",
        "    example_match = semantic_examples(query_str)\n",
        "    few_shot_template = '\\n'.join(map(str, example_match)).replace(\"{\",\"\").replace(\"}\",\"\")\n",
        "\n",
        "    template = left_template + few_shot_template + right_template\n",
        "    txt_sql_prompt_tmpl = PromptTemplate(template)\n",
        "\n",
        "    full_prompt = txt_sql_prompt_tmpl.format(\n",
        "        query_str=query_str,dialect = db.dialect,schema = schema\n",
        "    )\n",
        "    logger.info(\"****full_prompt****\"+str(full_prompt))\n",
        "    sql_query = llm.complete(full_prompt).text\n",
        "    logger.info(\"****sql_query****\"+str(sql_query))\n",
        "    try:\n",
        "        response = \"SQL Query:\"+sql_query+\"\\n\"+\"Execution Result:\"+\"\\n\"+str(pd.DataFrame(db._execute(sql_query)).to_markdown())\n",
        "    except:\n",
        "        response = \"SQL Query:\"+sql_query+\"\\n\"+\"Execution Result:\"+\"\\n\"+str(\"Unable to execute the query due to exception\")\n",
        "    logger.info(\"****response****\"+str(response))\n",
        "    return response\n",
        "\n",
        "def get_model_response(message, history):\n",
        "    natural_language = message\n",
        "    # response = return_response(natural_language)\n",
        "    response = return_response1(natural_language)\n",
        "    return response\n",
        "\n",
        "gr.ChatInterface(\n",
        "    get_model_response,\n",
        "    chatbot=gr.Chatbot(height=300),\n",
        "    textbox=gr.Textbox(placeholder=\"Ask me question/Natural language to generate SQL Query\", container=False, scale=7),\n",
        "    title=\"DataMart Place SQL Generator\",\n",
        "    description=\"Ask me question/Natural language to generate SQL Query\",\n",
        "    theme=\"soft\",\n",
        "    examples=[\"List of policies based on data\", \"List of policies based on status\"],\n",
        "    cache_examples=True,\n",
        "    submit_btn = \"Submit\",\n",
        "    stop_btn = \"Stop\",\n",
        "    retry_btn=\"Retry\",\n",
        "    undo_btn=\"Undo\",\n",
        "    clear_btn=\"Clear\",\n",
        ").launch()\n",
        "\n",
        "query_engine = NLSQLTableQueryEngine(\n",
        "    sql_database=db, tables=include_tables,\n",
        "    llm=llm\n",
        ")\n",
        "schema = db.table_info\n",
        "logger.info(\"loaded variables\")\n",
        "\n",
        "def return_db_prefix(**kwargs):\n",
        "    return \"RAW_DMP_SB_DEV_DMP\"\n",
        "\n",
        "def semantic_examples(**kwargs):\n",
        "    query_str = kwargs[\"query_str\"]\n",
        "    corpus = []\n",
        "    for i in range(0,len(examples)):\n",
        "        corpus.append(examples[i]['input'])\n",
        "    embedder = SentenceTransformer(r\"all-MiniLM-L6-v2\")\n",
        "    corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
        "    queries = [query_str]\n",
        "    example_match = []\n",
        "    for query in queries:\n",
        "        query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "        hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\n",
        "        hits = hits[0]\n",
        "        for hit in hits:\n",
        "            inner_dict = {}\n",
        "            inner_dict[\"input\"] = corpus[hit['corpus_id']]\n",
        "            for i in range(0,len(examples)):\n",
        "                if corpus[hit['corpus_id']] in examples[i]['input']:\n",
        "                    inner_dict[\"query\"] = examples[i]['query']\n",
        "            example_match.append(inner_dict)\n",
        "    return '\\n'.join(map(str, example_match)).replace(\"{\",\"\").replace(\"}\",\"\")\n",
        "\n",
        "def return_response(query_str):\n",
        "    logger.info(query_str)\n",
        "    template = prefix_template + suffix_template\n",
        "    txt_sql_prompt_tmpl = PromptTemplate(template,\n",
        "                                     function_mappings={\"few_shot_examples\": semantic_examples,\n",
        "                                                        \"table_prefix\": return_db_prefix}\n",
        "                                     )\n",
        "\n",
        "    query_engine.update_prompts(\n",
        "        {\"sql_retriever:text_to_sql_prompt\": txt_sql_prompt_tmpl}\n",
        "    )\n",
        "    logger.info(\"prompt****\"+str(query_engine.get_prompts()))\n",
        "    response = query_engine.query(query_str)\n",
        "    sql_query = response.metadata['sql_query']\n",
        "    try:\n",
        "        response = \"SQL Query:\"+sql_query+\"\\n\"+\"Execution Result:\"+\"\\n\"+str(pd.DataFrame(db._execute(sql_query)).to_markdown())\n",
        "    except:\n",
        "        response = \"SQL Query:\"+sql_query+\"\\n\"+\"Execution Result:\"+\"\\n\"+str(\"Unable to execute the query due to exception\")\n",
        "    logger.info(\"response****\"+str(response))\n",
        "    return response\n",
        "\n",
        "def return_response1(query_str):\n",
        "    logger.info(\"query_str***\"+str(query_str))\n",
        "    template = prefix_template + suffix_template\n",
        "    txt_sql_prompt_tmpl = PromptTemplate(template,\n",
        "                                     function_mappings={\"few_shot_examples\": semantic_examples,\n",
        "                                                        \"table_prefix\": return_db_prefix}\n",
        "                                     )\n",
        "    full_prompt = txt_sql_prompt_tmpl.format(\n",
        "        query_str=query_str,dialect = db.dialect,schema = schema\n",
        "    )\n",
        "    logger.info(\"full_prompt****\"+str(full_prompt))\n",
        "    sql_query = llm.complete(full_prompt).text\n",
        "    logger.info(\"sql_query****\"+str(sql_query))\n",
        "    try:\n",
        "        response = \"SQL Query:\"+sql_query+\"\\n\"+\"Execution Result:\"+\"\\n\"+str(pd.DataFrame(db._execute(sql_query)).to_markdown())\n",
        "    except:\n",
        "        response = \"SQL Query:\"+sql_query+\"\\n\"+\"Execution Result:\"+\"\\n\"+str(\"Unable to execute the query due to exception\")\n",
        "    logger.info(\"response***\"+str(response))\n",
        "    return response\n",
        "\n",
        "def get_model_response(message, history):\n",
        "    natural_language = message\n",
        "    # response = return_response(natural_language)\n",
        "    response = return_response1(natural_language)\n",
        "    return response\n",
        "\n",
        "gr.ChatInterface(\n",
        "    get_model_response,\n",
        "    chatbot=gr.Chatbot(height=300),\n",
        "    textbox=gr.Textbox(placeholder=\"Ask me question/Natural language to generate SQL Query\", container=False, scale=7),\n",
        "    title=\"DataMart Place SQL Generator\",\n",
        "    description=\"Ask me question/Natural language to generate SQL Query\",\n",
        "    theme=\"soft\",\n",
        "    examples=[\"List of policies based on data\", \"List of policies based on status\"],\n",
        "    cache_examples=True,\n",
        "    submit_btn = \"Submit\",\n",
        "    stop_btn = \"Stop\",\n",
        "    retry_btn=\"Retry\",\n",
        "    undo_btn=\"Undo\",\n",
        "    clear_btn=\"Clear\",\n",
        ").launch()\n",
        "\n",
        "\n",
        "from llama_index.agent.openai import OpenAIAgent\n",
        "# from llama_index.core.tools.google import GmailToolSpec\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def normal_regular_prompt(query):\n",
        "    return li_llm.complete(query)\n",
        "from llama_index.core.agent import ReActAgent\n",
        "function_tool1 = FunctionTool.from_defaults(fn=web_prompt_tool)\n",
        "function_tool2 = FunctionTool.from_defaults(fn=normal_regular_prompt)\n",
        "tools = [function_tool1,function_tool2]\n",
        "agent = ReActAgent.from_tools(tools, llm=li_llm, verbose=True)\n",
        "# response = agent.chat(\"Open source LLM models\")\n",
        "\n",
        "\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "react_system_prompt = PromptTemplate(PROMPT)\n",
        "agent.get_prompts()\n",
        "\n",
        "agent.update_prompts({\"agent_worker:system_prompt\": react_system_prompt})\n",
        "agent.reset()\n",
        "response = agent.chat(\"text generation models from huggingface sources\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "tCh1p-OAXf6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from IPython.display import Markdown, display\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    [], storage_context=storage_context\n",
        ")\n",
        "\n",
        "from llama_index.core.schema import TextNode\n",
        "nodes = []\n",
        "for i in range(0,len(data)):\n",
        "    node1 = TextNode(\n",
        "            text=data[i].page_content,\n",
        "            metadata=data[i].metadata,\n",
        "            embedding=embed_model.get_text_embedding(data[i].page_content)\n",
        "        )\n",
        "\n",
        "    nodes.append(node1)\n",
        "\n",
        "index.insert_nodes(nodes)\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "from llama_index.core.prompts import LangchainPromptTemplate\n",
        "\n",
        "lc_prompt_tmpl = LangchainPromptTemplate(\n",
        "    template=PROMPT,\n",
        "    template_var_mappings={\"query_str\": \"question\", \"context_str\": \"context\"},\n",
        ")\n",
        "\n",
        "query_engine.update_prompts(\n",
        "    {\"response_synthesizer:text_qa_template\": lc_prompt_tmpl}\n",
        ")\n",
        "prompts_dict = query_engine.get_prompts()\n",
        "display_prompt_dict(prompts_dict)\n",
        "\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model)\n",
        "\n",
        "query_engine = RetrieverQueryEngine.from_args(\n",
        "    index.as_retriever(),\n",
        "    # llm=llm,\n",
        "    service_context=service_context,\n",
        ")\n",
        "response = query_engine.query(\"what are the sections mentioned?\")\n",
        "print(str(response))\n",
        "\n",
        "\n",
        "from llama_index.core.query_engine import CitationQueryEngine\n",
        "query_engine = CitationQueryEngine.from_args(\n",
        "    index,\n",
        "    similarity_top_k=5,\n",
        "    # here we can control how granular citation sources are, the default is 512\n",
        "    # citation_chunk_size=512,\n",
        "    service_context=service_context,\n",
        ")\n",
        "response = query_engine.query(\"what are the sections mentioned?\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "NertfBovXf3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, Tool, create_react_agent\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "prompt = PromptTemplate.from_template(PROMPT)\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent,\n",
        "                               tools=tools,\n",
        "                               verbose=True,\n",
        "                                max_iterations=5,\n",
        "                                return_intermediate_steps=True,\n",
        "                                handle_parsing_errors=True,)\n",
        "cfg = RunnableConfig()\n",
        "# cfg[\"callbacks\"] = [st_callback]\n",
        "answer = agent_executor.invoke({\"input\": \"List of open source text classification models from huggingface\"}, cfg)\n",
        "\n",
        "import re\n",
        "from typing import List, Union\n",
        "\n",
        "from langchain.agents import (\n",
        "    AgentExecutor,\n",
        "    AgentOutputParser,\n",
        "    LLMSingleActionAgent,\n",
        "    Tool,\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import StringPromptTemplate\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "# Set up a prompt template\n",
        "class CustomPromptTemplate(StringPromptTemplate):\n",
        "    # The template to use\n",
        "    template: str\n",
        "    # The list of tools available\n",
        "    tools: List[Tool]\n",
        "\n",
        "    def format(self, **kwargs) -> str:\n",
        "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
        "        # Format them in a particular way\n",
        "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
        "        thoughts = \"\"\n",
        "        for action, observation in intermediate_steps:\n",
        "            thoughts += action.log\n",
        "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
        "        # Set the agent_scratchpad variable to that value\n",
        "        kwargs[\"agent_scratchpad\"] = thoughts\n",
        "        # Create a tools variable from the list of tools provided\n",
        "        kwargs[\"tools\"] = \"\\n\".join(\n",
        "            [f\"{tool.name}: {tool.description}\" for tool in self.tools]\n",
        "        )\n",
        "        # Create a list of tool names for the tools provided\n",
        "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
        "        return self.template.format(**kwargs)\n",
        "prompt = CustomPromptTemplate(\n",
        "    template=PROMPT,\n",
        "    tools=tools,\n",
        "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
        "    # This includes the `intermediate_steps` variable because that is needed\n",
        "    input_variables=[\"input\", \"intermediate_steps\"],\n",
        ")\n",
        "class CustomOutputParser(AgentOutputParser):\n",
        "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "        # Check if agent should finish\n",
        "        if \"Final Answer:\" in llm_output:\n",
        "            return AgentFinish(\n",
        "                # Return values is generally always a dictionary with a single `output` key\n",
        "                # It is not recommended to try anything else at the moment :)\n",
        "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
        "                log=llm_output,\n",
        "            )\n",
        "        # Parse out the action and action input\n",
        "        regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n",
        "        match = re.search(regex, llm_output, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
        "        action = match.group(1).strip()\n",
        "        action_input = match.group(2)\n",
        "        # Return the action and action input\n",
        "        return AgentAction(\n",
        "            tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output\n",
        "        )\n",
        "\n",
        "output_parser = CustomOutputParser()\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "tool_names = [tool.name for tool in tools]\n",
        "agent = LLMSingleActionAgent(\n",
        "    llm_chain=llm_chain,\n",
        "    output_parser=output_parser,\n",
        "    stop=[\"\\nObservation:\"],\n",
        "    allowed_tools=tool_names,\n",
        "    verbose=True,\n",
        "    max_iterations=5,\n",
        "    return_intermediate_steps=True,\n",
        "    handle_parsing_errors=True,\n",
        ")\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent, tools=tools, verbose=True,\n",
        "    max_iterations=5,\n",
        "    handle_parsing_errors=True,\n",
        ")\n",
        "response = agent_executor.run(\"open source text classification models from huggingface\")\n",
        "\n",
        "\n",
        "from langchain.agents import AgentExecutor, Tool, ZeroShotAgent\n",
        "prompt = ZeroShotAgent.create_prompt(\n",
        "    tools,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"input\",\n",
        "                    #  \"chat_history\",\n",
        "                     \"agent_scratchpad\"\n",
        "                     ],\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\n",
        "agent_chain = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent, tools=tools, verbose=True,\n",
        "    # memory=memory,\n",
        "    return_intermediate_steps=True,\n",
        ")\n",
        "\n",
        "\n",
        "from llama_index.agent.openai import OpenAIAgent\n",
        "# from llama_index.core.tools.google import GmailToolSpec\n",
        "from llama_index.core.tools import FunctionTool\n",
        "\n",
        "def normal_regular_prompt(query):\n",
        "    return llm.complete(query)\n",
        "from llama_index.core.agent import ReActAgent\n",
        "function_tool1 = FunctionTool.from_defaults(fn=web_prompt_tool)\n",
        "function_tool2 = FunctionTool.from_defaults(fn=normal_regular_prompt)\n",
        "tools = [function_tool1,function_tool2]\n",
        "agent = ReActAgent.from_tools(tools, llm=llm, verbose=True)\n",
        "response = agent.chat(\"Open source LLM models\")\n",
        "\n",
        "react_system_prompt = PromptTemplate(PROMPT)\n",
        "agent.get_prompts()\n",
        "\n",
        "agent.update_prompts({\"agent_worker:system_prompt\": react_system_prompt})\n",
        "agent.reset()\n",
        "response = agent.chat(\"text generation models from huggingface sources\")\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "U0uXYOa2cVN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder, SentenceTransformersTextEmbedder\n",
        "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
        "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
        "\n",
        "doc_embedder = SentenceTransformersDocumentEmbedder(model=model_name)\n",
        "doc_embedder.warm_up()\n",
        "docs_with_embeddings = doc_embedder.run(documents)[\"documents\"]\n",
        "\n",
        "doc_store = InMemoryDocumentStore()\n",
        "doc_store.write_documents(docs_with_embeddings)\n",
        "retriever = InMemoryEmbeddingRetriever(doc_store,top_k=top_k)\n",
        "\n",
        "text_embedder = SentenceTransformersTextEmbedder(model=model_name)\n",
        "text_embedder.warm_up()\n",
        "query_embedding = text_embedder.run(query)[\"embedding\"]\n",
        "\n",
        "result = retriever.run(query_embedding=query_embedding)\n",
        "\n",
        "print(result[\"documents\"])\n",
        "\n",
        "ctx = []\n",
        "for i in range(0,len(result[\"documents\"])):\n",
        "    ctx.append(result[\"documents\"][i].content)\n",
        "ctx = '\\n'.join(map(str, ctx))\n",
        "\n",
        "suffix_template = \"\"\"Question: {{ question }}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "final_template = prefix+\"\\n\"+ctx+\"\\n\"+suffix_template\n",
        "final_template\n",
        "\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.generators import AzureOpenAIGenerator\n",
        "import os\n",
        "template = final_template\n",
        "prompt_builder = PromptBuilder(template=template)\n",
        "from haystack import Pipeline\n",
        "pipe = Pipeline()\n",
        "pipe.add_component(\"prompt_builder\", PromptBuilder(template=template))\n",
        "pipe.add_component(\"llm\", AzureOpenAIGenerator\n",
        "\n",
        "pipe.connect(\"prompt_builder\", \"llm\")\n",
        "result = pipe.run({\n",
        "    \"prompt_builder\": {\n",
        "        \"question\": query\n",
        "    }\n",
        "})\n",
        "result['llm']['replies']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e1iRfEWmdzvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, Tool, create_react_agent\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "prompt = PromptTemplate.from_template(PROMPT)\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent,\n",
        "                               tools=tools,\n",
        "                               verbose=True,\n",
        "                                max_iterations=5,\n",
        "                                return_intermediate_steps=True,\n",
        "                                handle_parsing_errors=True,)\n",
        "cfg = RunnableConfig()\n",
        "# cfg[\"callbacks\"] = [st_callback]\n",
        "answer = agent_executor.invoke({\"input\": \"List of open source text classification models from huggingface\"}, cfg)\n",
        "\n",
        "\n",
        "import re\n",
        "from typing import List, Union\n",
        "\n",
        "from langchain.agents import (\n",
        "    AgentExecutor,\n",
        "    AgentOutputParser,\n",
        "    LLMSingleActionAgent,\n",
        "    Tool,\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import StringPromptTemplate\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "# Set up a prompt template\n",
        "class CustomPromptTemplate(StringPromptTemplate):\n",
        "    # The template to use\n",
        "    template: str\n",
        "    # The list of tools available\n",
        "    tools: List[Tool]\n",
        "\n",
        "    def format(self, **kwargs) -> str:\n",
        "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
        "        # Format them in a particular way\n",
        "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
        "        thoughts = \"\"\n",
        "        for action, observation in intermediate_steps:\n",
        "            thoughts += action.log\n",
        "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
        "        # Set the agent_scratchpad variable to that value\n",
        "        kwargs[\"agent_scratchpad\"] = thoughts\n",
        "        # Create a tools variable from the list of tools provided\n",
        "        kwargs[\"tools\"] = \"\\n\".join(\n",
        "            [f\"{tool.name}: {tool.description}\" for tool in self.tools]\n",
        "        )\n",
        "        # Create a list of tool names for the tools provided\n",
        "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
        "        return self.template.format(**kwargs)\n",
        "prompt = CustomPromptTemplate(\n",
        "    template=PROMPT,\n",
        "    tools=tools,\n",
        "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
        "    # This includes the `intermediate_steps` variable because that is needed\n",
        "    input_variables=[\"input\", \"intermediate_steps\"],\n",
        ")\n",
        "class CustomOutputParser(AgentOutputParser):\n",
        "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "        # Check if agent should finish\n",
        "        if \"Final Answer:\" in llm_output:\n",
        "            return AgentFinish(\n",
        "                # Return values is generally always a dictionary with a single `output` key\n",
        "                # It is not recommended to try anything else at the moment :)\n",
        "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
        "                log=llm_output,\n",
        "            )\n",
        "        # Parse out the action and action input\n",
        "        regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n",
        "        match = re.search(regex, llm_output, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
        "        action = match.group(1).strip()\n",
        "        action_input = match.group(2)\n",
        "        # Return the action and action input\n",
        "        return AgentAction(\n",
        "            tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output\n",
        "        )\n",
        "\n",
        "output_parser = CustomOutputParser()\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "tool_names = [tool.name for tool in tools]\n",
        "agent = LLMSingleActionAgent(\n",
        "    llm_chain=llm_chain,\n",
        "    output_parser=output_parser,\n",
        "    stop=[\"\\nObservation:\"],\n",
        "    allowed_tools=tool_names,\n",
        "    verbose=True,\n",
        "    max_iterations=5,\n",
        "    return_intermediate_steps=True,\n",
        "    handle_parsing_errors=True,\n",
        ")\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent, tools=tools, verbose=True,\n",
        "    max_iterations=5,\n",
        "    handle_parsing_errors=True,\n",
        ")\n",
        "\n",
        "agent_executor.run(\"open source text classification models from huggingface\")"
      ],
      "metadata": {
        "id": "sZCH0tMWeZEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "document_query_engine = [\n",
        "    QueryEngineTool(\n",
        "        query_engine=query_engine,\n",
        "        metadata=ToolMetadata(\n",
        "            name=\"Document Query Engine\",\n",
        "            description=(\n",
        "                \"Retrieve the answers from the documents\"\n",
        "            ),\n",
        "        ),\n",
        "    ),\n",
        "]\n",
        "\n",
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "query_engine_tools = []\n",
        "query_engine_tools.append(\n",
        "        QueryEngineTool(\n",
        "            query_engine=query_engine,\n",
        "            metadata=ToolMetadata(\n",
        "                name=f\"claims_files\",\n",
        "                description=(\n",
        "                    \"claims file analysis\"\n",
        "                ),\n",
        "            ),\n",
        "        )\n",
        "    )\n",
        "from llama_index.core.agent import AgentRunner\n",
        "# from llama_index.agent import OpenAIAgent\n",
        "from llama_index.agent.openai import OpenAIAgentWorker, OpenAIAgent\n",
        "# from llama_index.agent.openai import OpenAIAgentWorker\n",
        "\n",
        "openai_step_engine = OpenAIAgentWorker.from_tools(\n",
        "    query_engine_tools, llm=llm, verbose=True\n",
        ")\n",
        "agent = AgentRunner(openai_step_engine)\n",
        "# # alternative\n",
        "# agent = OpenAIAgent.from_tools(query_engine_tools, llm=llm, verbose=True)\n",
        "\n",
        "\n",
        "# query_engine_tools[0].query_engine.query(\"sections mentioned\")\n",
        "\n",
        "\n",
        "from llama_index.core.agent import AgentRunner, ReActAgent\n",
        "agent = ReActAgent.from_tools(\n",
        "    query_engine_tools, llm=llm, verbose=True, max_iterations=20\n",
        ")\n",
        "\n",
        "\n",
        "response = agent.chat(\n",
        "    \"what are the sections mentioned in the document?\"\n",
        ")\n",
        "\n",
        "\n",
        "task = agent.create_task(\"what are the sections mentioned in the document?\")\n",
        "step_output = agent.run_step(task.task_id)\n",
        "step_output = agent.run_step(task.task_id, input=\"what are the sections mentioned in the document?\")\n",
        "print(step_output.is_last)\n",
        "response = agent.finalize_response(task.task_id)\n",
        "\n",
        "\n",
        "new_query_engine = [query_engine_tools[0],query_engine_tools[1][0],query_engine_tools[1][1],query_engine_tools[1][2]]\n",
        "new_query_engine\n",
        "from llama_index.core.agent import AgentRunner, ReActAgent\n",
        "agent = ReActAgent.from_tools(\n",
        "    new_query_engine, llm=llm, verbose=True, max_iterations=20\n",
        ")\n",
        "\n",
        "\n",
        "response = agent.chat(\n",
        "    \"what are the sections mentioned in the document?\"\n",
        ")\n",
        "\n",
        "from llama_index.core.prompts import LangchainPromptTemplate\n",
        "\n",
        "lc_prompt_tmpl = LangchainPromptTemplate(\n",
        "    template=PROMPT,\n",
        "    template_var_mappings={\"query_str\": \"question\", \"context_str\": \"context\"},\n",
        ")\n",
        "\n",
        "query_engine.update_prompts(\n",
        "    {\"response_synthesizer:text_qa_template\": lc_prompt_tmpl}\n",
        ")\n",
        "\n",
        "\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "react_system_header_str = \"\"\"\\\n",
        "\n",
        "You are designed to help with a variety of tasks, from answering questions \\\n",
        "    to providing summaries to other types of analyses.\n",
        "\n",
        "## Tools\n",
        "You have access to a wide variety of tools. You are responsible for using\n",
        "the tools in any sequence you deem appropriate to complete the task at hand.\n",
        "This may require breaking the task into subtasks and using different tools\n",
        "to complete each subtask.\n",
        "\n",
        "You have access to the following tools:\n",
        "{tool_desc}\n",
        "\n",
        "## Output Format\n",
        "To answer the question, please use the following format.\n",
        "\n",
        "```\n",
        "Thought: I need to use a tool to help me answer the question.\n",
        "Action: tool name (one of {tool_names}) if using a tool.\n",
        "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n",
        "```\n",
        "\n",
        "Please ALWAYS start with a Thought.\n",
        "\n",
        "Please use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n",
        "\n",
        "If this format is used, the user will respond in the following format:\n",
        "\n",
        "```\n",
        "Observation: tool response\n",
        "```\n",
        "\n",
        "You should keep repeating the above format until you have enough information\n",
        "to answer the question without using any more tools. At that point, you MUST respond\n",
        "in the one of the following two formats:\n",
        "\n",
        "```\n",
        "Thought: I can answer without using any more tools.\n",
        "Answer: [your answer here]\n",
        "```\n",
        "\n",
        "```\n",
        "Thought: I cannot answer the question with the provided tools.\n",
        "Answer: Sorry, I cannot answer your query.\n",
        "```\n",
        "\n",
        "## Additional Rules\n",
        "- The answer MUST contain a sequence of bullet points that explain how you arrived at the answer. This can include aspects of the previous conversation history.\n",
        "- You MUST obey the function signature of each tool. Do NOT pass in no arguments if the function expects arguments.\n",
        "\n",
        "## Current Conversation\n",
        "Below is the current conversation consisting of interleaving human and assistant messages.\n",
        "\n",
        "\"\"\"\n",
        "react_system_prompt = PromptTemplate(react_system_header_str)\n",
        "agent.get_prompts()\n",
        "\n",
        "agent.update_prompts({\"agent_worker:system_prompt\": react_system_prompt})\n",
        "\n",
        "agent.reset()\n",
        "response = agent.chat(\"text generation models from huggingface sources\")\n",
        "print(response)\n",
        "\n",
        "tasks = agent.list_tasks()\n",
        "print(len(tasks))\n",
        "\n",
        "completed_steps = agent.get_completed_steps(task_state.task.task_id)\n",
        "len(completed_steps)\n",
        "# 3\n",
        "completed_steps[0]\n",
        "\n",
        "\n",
        "for idx in range(len(completed_steps)):\n",
        "    print(f\"Step {idx}\")\n",
        "    print(f\"Response: {completed_steps[idx].output.response}\")\n",
        "    print(f\"Sources: {completed_steps[idx].output.sources}\")\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\", output_key=\"answer\", return_messages=True\n",
        ")\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    # db.as_retriever(search_kwargs={\"k\": 5}),\n",
        "    retriever,\n",
        "    return_source_documents=True,\n",
        "    memory=memory,\n",
        "    verbose=False,\n",
        "    combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
        ")\n",
        "\n",
        "question = \"I'm on the phone with a broker right now, and he's asking about the limit on one of his Cyber ERM policies. The account is up for renewal soon and the revenue is $500m. The current limit is $5M, but they'd like a limit of $15M. Can I proceed with his request?\"\n",
        "\n",
        "result = qa_chain.invoke({\"question\": question})\n",
        "result\n",
        "\n",
        "from langchain.agents import Tool\n",
        "from langchain.agents import AgentType, Tool, initialize_agent\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "tools = [\n",
        "        # Tool(\n",
        "        #     name=\"Document Question and Answering\",\n",
        "        #     func=doc_qa.run,\n",
        "        #     description=\"Useful when the user is asking questions to extract answers around the documents\",\n",
        "        # ),\n",
        "        Tool(\n",
        "        name=\"Web/Website Prompt Answering\",\n",
        "        func=web_prompt_answering.run,\n",
        "        description=\"Useful when the user is asking web/website prompts when this is not categorized under regular/normal/ordinary prompts\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Regular/Normal/Ordinary Prompt Answering\",\n",
        "        func=regular_prompt_answering.run,\n",
        "        description=\"Useful when the user is asking regular/normal/ordinary prompts when this is not categorized to web prompts/website prompts\"\n",
        "    ),\n",
        "    # Tool(\n",
        "    #     name=\"Summarized Prompt/Document Answering\",\n",
        "    #     func=summarize_chain_answering.run,\n",
        "    #     description=\"Useful when the user is asking any queries to summarize the document/prompt\"\n",
        "    # ),\n",
        "    ]\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools, llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    max_iterations=5,\n",
        "    # max_execution_time=1,\n",
        "    return_intermediate_steps=True,\n",
        "    memory=ConversationBufferMemory(memory_key=\"chat_history\", input_key='input', output_key=\"output\", return_messages=True),\n",
        ")\n",
        "\n",
        "\n",
        "agent(\n",
        "        {\n",
        "            \"input\": query\n",
        "        }\n",
        "    )"
      ],
      "metadata": {
        "id": "rpCsTGKle5FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "prompt = PromptTemplate(input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'], template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True,return_intermediate_steps=True,\n",
        "                               memory=ConversationBufferMemory(memory_key=\"chat_history\", input_key='input', output_key=\"output\", return_messages=True),)\n",
        "result = agent_executor.invoke({\"input\": \"what are the coverages covered of the document?\"})\n",
        "\n",
        "agent_executor.invoke({\"input\": \"what are the text classification models in huggingface?\"})\n"
      ],
      "metadata": {
        "id": "4zse4p34gsVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"\"\"Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\"\"\"\n",
        "suffix = \"\"\"Begin!\"\n",
        "\n",
        "{chat_history}\n",
        "Question: {input}\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = ZeroShotAgent.create_prompt(\n",
        "    tools,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"],\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\n",
        "agent_chain = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent, tools=tools, verbose=True, memory=memory,return_intermediate_steps=True,\n",
        ")\n",
        "agent_chain.invoke(input=\"what are the coverages covered in the document?\")\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools, llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    max_iterations=2,\n",
        "    return_intermediate_steps=True,\n",
        "    memory=ConversationBufferMemory(memory_key=\"chat_history\", input_key='input', output_key=\"output\", return_messages=True),\n",
        ")\n",
        "agent({\"input\": \"what are the coverages covered of the document?\"})\n",
        "\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "prompt = PromptTemplate(input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'], template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')\n",
        "\n",
        "agent = create_react_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True,return_intermediate_steps=True,\n",
        "                               memory=ConversationBufferMemory(memory_key=\"chat_history\", input_key='input', output_key=\"output\", return_messages=True),)\n",
        "result = agent_executor.invoke({\"input\": \"what are the coverages covered of the document?\"})\n",
        "\n",
        "\n",
        "from langchain.tools.retriever import create_retriever_tool\n",
        "\n",
        "tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"Document_Question_Answering\",\n",
        "    \"Useful when the user is asking questions to extract answers around the documents\",\n",
        ")\n",
        "tools = [tool]\n",
        "\n",
        "from langchain_core.prompts.chat import  ChatPromptTemplate\n",
        "import typing\n",
        "import langchain_core\n",
        "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate,HumanMessagePromptTemplate,PromptTemplate,MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'],\n",
        "                            input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage,\n",
        "                                                                                  langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]], 'agent_scratchpad': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')), MessagesPlaceholder(variable_name='chat_history', optional=False), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
        "\n",
        "\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "agent = create_openai_tools_agent(llm, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools,verbose=True, return_intermediate_steps=True,\n",
        "                               memory=memory\n",
        "                            )\n",
        "\n",
        "# We can now try it out!\n",
        "\n",
        "result = agent_executor.invoke({\"input\": \"what are the coverages covered in the document?\"})\n",
        "\n",
        "prefix = \"\"\"Have a conversation with a human, answering the following questions as best you can. You have access to the following tools:\"\"\"\n",
        "suffix = \"\"\"Begin!\"\n",
        "\n",
        "{chat_history}\n",
        "Question: {input}\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "prompt = ZeroShotAgent.create_prompt(\n",
        "    tools,\n",
        "    prefix=prefix,\n",
        "    suffix=suffix,\n",
        "    input_variables=[\"input\", \"chat_history\", \"agent_scratchpad\"],\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools, verbose=True)\n",
        "agent_chain = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent, tools=tools, verbose=True, memory=memory,return_intermediate_steps=True,\n",
        ")\n",
        "\n",
        "result = agent_chain.invoke(input=\"what are the coverages covered in the document?\")\n"
      ],
      "metadata": {
        "id": "YUJpGsmRhUhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage\n",
        "message = HumanMessage(\n",
        "        content=\"hi\"\n",
        "    )\n",
        "llm([message]).content\n",
        "\n",
        "\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferMemory(memory_key=\"history\", input_key='input', return_messages=True)\n",
        ")\n",
        "\n",
        "resp = conversation.predict(input=\"Hi there!\")\n",
        "resp\n",
        "\n",
        "resp = conversation.predict(input=\"Hi how are you!\")\n",
        "\n",
        "# from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "# llm = ChatOpenAI()\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "            \"You are a nice chatbot having a conversation with a human.\"\n",
        "        ),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "    ]\n",
        ")\n",
        "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
        "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name.\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conversation = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "respons = conversation({\"question\": \"hi\"})\n",
        "respons\n",
        "\n",
        "respons = conversation({\"question\": \"how are you doing?\"})\n",
        "respons\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools, llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    max_iterations=2,\n",
        "    return_intermediate_steps=True,\n",
        "    memory=ConversationBufferMemory(memory_key=\"chat_history\", input_key='input', output_key=\"output\", return_messages=True),\n",
        ")\n",
        "\n",
        "agent(\n",
        "        {\n",
        "            \"input\": \"what is the summarized content of the document attached?\"\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools, llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    # agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    max_iterations=2,\n",
        "    # return_direct=True,\n",
        "    # intermediate_steps=True,\n",
        "    # intermediate_steps=['Action Input', 'Observation','Action'],\n",
        "    return_intermediate_steps=True,\n",
        "    # memory=ConversationBufferMemory(memory_key=\"chat_history\", input_key='input', output_key=\"output\", return_messages=True),\n",
        "    # intermediate_steps=['Action Input', 'Observation','Action'],\n",
        "    # memory=ConversationBufferMemory(memory_key=\"chat_history\", input_key='input', output_key=\"output\",return_messages=True),\n",
        "    # memory=ConversationSummaryBufferMemory(memory_key=\"chat_history\", llm=llm,input_key='input', output_key=\"output\",return_messages=True),\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools, llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    # agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    max_iterations=2,\n",
        "    return_intermediate_steps=True\n",
        "    # return_direct=True,\n",
        "    # intermediate_steps=True,\n",
        "    # intermediate_steps=['Action Input', 'Observation'],\n",
        "    # memory=ConversationBufferMemory(memory_key=\"chat_history\", input_key='input', output_key=\"output\", return_messages=True),\n",
        "    # intermediate_steps=['Action Input', 'Observation','Action'],\n",
        "    # memory=ConversationBufferMemory(memory_key=\"chat_history\", input_key='input', output_key=\"output\",return_messages=True),\n",
        "    # memory=ConversationSummaryBufferMemory(memory_key=\"chat_history\", llm=llm,input_key='input', output_key=\"output\",return_messages=True),\n",
        "\n",
        ")\n",
        "\n",
        "# memory = ConversationSummaryBufferMemory(\n",
        "#     return_messages=True, llm=llm,\n",
        "#     # max_token_limit=150,\n",
        "#     memory_key=\"chat_history\"\n",
        "#     )\n",
        "\n",
        "# set the output key so that memory doesn't error on save\n",
        "# memory.output_key = \"output\"\n",
        "\n",
        "# # You an input a previously saved agent state. like this:\n",
        "# state = [{'type': 'ai', 'data': {\n",
        "#     'content': 'Nice to meet you, Tim!', 'additional_kwargs': {}}}]\n",
        "\n",
        "# memory.chat_memory.messages = messages_from_dict(state)\n",
        "\n",
        "# agent_chain = initialize_agent(\n",
        "#     tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory, intermediate_steps=True)\n",
        "\n",
        "# MESSAGE = \"What is currently the most popular web browser?\"\n",
        "\n",
        "# with get_openai_callback() as cb:\n",
        "\n",
        "#     out = agent_chain(\n",
        "#         inputs=[\"what is the summarized content of the document attached?\"])\n",
        "\n",
        "\n",
        "# The output dict will also contain a 'intermediate_steps' key.\n",
        "\n",
        "\n",
        "result = agent.run(\n",
        "    \"what is the summarized content of the document attached?\",\n",
        ")\n",
        "\n",
        "from langchain.agents import AgentExecutor\n",
        "agent_executor = AgentExecutor(\n",
        "    agent=agent,\n",
        "    tools=tools,\n",
        "    verbose=True,\n",
        "    intermediate_steps=True\n",
        ")\n",
        "response = agent_executor.invoke({\"input\": \"what is the summarized content of the document attached?\"})\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools, llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    # agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
        "    verbose=True,\n",
        "    max_iterations=2,\n",
        "    # return_direct=True,\n",
        "    # intermediate_steps=True,\n",
        "    # intermediate_steps=['Action Input', 'Observation','Action'],\n",
        "    return_intermediate_steps=True,\n",
        "    # memory=ConversationBufferMemory(memory_key=\"chat_history\", input_key='input', output_key=\"output\", return_messages=True),\n",
        "    # intermediate_steps=['Action Input', 'Observation','Action'],\n",
        "    # memory=ConversationBufferMemory(memory_key=\"chat_history\", input_key='input', output_key=\"output\",return_messages=True),\n",
        "    # memory=ConversationSummaryBufferMemory(memory_key=\"chat_history\", llm=llm,input_key='input', output_key=\"output\",return_messages=True),\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "9x1hJe7Ki5QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.query_engine import CustomQueryEngine\n",
        "from llama_index.retrievers import BaseRetriever\n",
        "from llama_index import get_response_synthesizer\n",
        "from llama_index.response_synthesizers import BaseSynthesizer\n",
        "\n",
        "class RAGQueryEngine(CustomQueryEngine):\n",
        "    \"\"\"RAG Query Engine.\"\"\"\n",
        "\n",
        "    retriever: BaseRetriever\n",
        "    response_synthesizer: BaseSynthesizer\n",
        "\n",
        "    def custom_query(self, query_str: str):\n",
        "        nodes = self.retriever.retrieve(query_str)\n",
        "        response_obj = self.response_synthesizer.synthesize(query_str, nodes)\n",
        "        return response_obj\n",
        "\n",
        "# from llama_index.llms.openai import OpenAI\n",
        "from llama_index.llms.azure_openai import AzureOpenAI\n",
        "from llama_index import PromptTemplate\n",
        "\n",
        "qa_prompt = PromptTemplate(\n",
        "    \"Context information is below.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Given the context information and not prior knowledge, \"\n",
        "    \"answer the query.\\n\"\n",
        "    \"Query: {query_str}\\n\"\n",
        "    \"Answer: \"\n",
        ")\n",
        "\n",
        "\n",
        "class RAGStringQueryEngine(CustomQueryEngine):\n",
        "    \"\"\"RAG String Query Engine.\"\"\"\n",
        "\n",
        "    retriever: BaseRetriever\n",
        "    response_synthesizer: BaseSynthesizer\n",
        "    llm: AzureOpenAI\n",
        "    qa_prompt: PromptTemplate\n",
        "\n",
        "    def custom_query(self, query_str: str):\n",
        "        nodes = self.retriever.retrieve(query_str)\n",
        "\n",
        "        context_str = \"\\n\\n\".join([n.node.get_content() for n in nodes])\n",
        "        response = self.llm.complete(\n",
        "            qa_prompt.format(context_str=context_str, query_str=query_str)\n",
        "        )\n",
        "\n",
        "        return str(response)\n",
        "\n",
        "synthesizer = get_response_synthesizer(response_mode=\"compact\",service_context=service_context)\n",
        "query_engine = RAGQueryEngine(\n",
        "    retriever=index.as_retriever(), response_synthesizer=synthesizer\n",
        ")\n",
        "response = query_engine.query(\"What are the sections mentioned?\")\n",
        "print(str(response))\n",
        "\n",
        "# llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "synthesizer = get_response_synthesizer(response_mode=\"compact\",service_context=service_context)\n",
        "query_engine = RAGStringQueryEngine(\n",
        "    retriever=index.as_retriever(),\n",
        "    response_synthesizer=synthesizer,\n",
        "    llm=llm,\n",
        "    qa_prompt=qa_prompt,\n",
        ")\n",
        "response = query_engine.query(\"What are the sections mentioned?\")\n",
        "print(str(response))\n",
        "\n",
        "\n",
        "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
        "query_engine_tools = []\n",
        "query_engine_tools.append(\n",
        "        QueryEngineTool(\n",
        "            query_engine=query_engine,\n",
        "            metadata=ToolMetadata(\n",
        "                name=f\"claims_files\",\n",
        "                description=(\n",
        "                    \"claims file analysis\"\n",
        "                ),\n",
        "            ),\n",
        "        )\n",
        "    )\n",
        "from llama_index.agent import AgentRunner\n",
        "# from llama_index.agent import OpenAIAgent\n",
        "from llama_index.agent import OpenAIAgentWorker, OpenAIAgent\n",
        "# from llama_index.agent.openai import OpenAIAgentWorker\n",
        "\n",
        "openai_step_engine = OpenAIAgentWorker.from_tools(\n",
        "    query_engine_tools, llm=llm, verbose=True\n",
        ")\n",
        "agent = AgentRunner(openai_step_engine)\n",
        "# # alternative\n",
        "# agent = OpenAIAgent.from_tools(query_engine_tools, llm=llm, verbose=True)\n",
        "\n",
        "\n",
        "from llama_index.agent import AgentRunner, ReActAgent\n",
        "agent = ReActAgent.from_tools(\n",
        "    query_engine_tools, llm=llm, verbose=True, max_iterations=20\n",
        ")\n",
        "\n",
        "\n",
        "response = agent.chat(\n",
        "    \"what are the sections mentioned in the document?\"\n",
        ")\n",
        "\n",
        "prompt_dict = agent.get_prompts()\n",
        "for k, v in prompt_dict.items():\n",
        "    print(f\"Prompt: {k}\\n\\nValue: {v.template}\")\n",
        "\n",
        "react_system_prompt = PromptTemplate(react_system_header_str)\n",
        "agent.update_prompts({\"agent_worker:system_prompt\": react_system_prompt})\n",
        "agent.reset()\n",
        "\n",
        "\n",
        "agent.chat(\"what are the sections mentioned?\")\n",
        "\n",
        "agent.create_task(\"what are the sections mentioned in the document?\")\n",
        "\n",
        "agent.run_step(task.task_id)\n",
        "\n",
        "agent.run_step(task.task_id, input=\"what are the sections mentioned in the document?\")\n",
        "\n",
        "agent.finalize_response(task.task_id)\n",
        "\n",
        "query_engine_tools = [tool1, tool2]\n",
        "\n",
        "agent = ReActAgent.from_tools(\n",
        "    query_engine_tools, llm=llm, verbose=True, max_iterations=20\n",
        ")\n",
        "\n",
        "agent.chat(\n",
        "    \"what are the sections mentioned in the document from claims_files2?\"\n",
        ")\n",
        "\n",
        "\n",
        "from langchain.agents import AgentType, Tool, initialize_agent\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True,\n",
        "    max_iterations=2\n",
        ")\n",
        "result = agent.run(\n",
        "    \"what is the summarization of the resoruces that are coming out from web?\",\n",
        ")"
      ],
      "metadata": {
        "id": "V9qXarlqi5Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "You are an expert and an intelligent question answering engine in the insurance industry space.\n",
        "You are provided multiple context items that are related to the prompt you have to answer.\n",
        "\n",
        "Pre-Requsities:\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "Even though the question being asked in other languages, please return the answers only in English language independent of the languages the questions being asked.\n",
        "Always traslate and return the answer responses only in <ENGLISH>.\n",
        "\n",
        "```\n",
        "{context}\n",
        "```\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "\n",
        "\n",
        "from __future__ import annotations\n",
        "from typing import Dict, Optional, Sequence\n",
        "from langchain.schema import Document\n",
        "from langchain.pydantic_v1 import Extra, root_validator\n",
        "\n",
        "from langchain.callbacks.manager import Callbacks\n",
        "from langchain.retrievers.document_compressors.base import BaseDocumentCompressor\n",
        "\n",
        "from sentence_transformers import CrossEncoder\n",
        "# from config import bge_reranker_large\n",
        "\n",
        "class BgeRerank(BaseDocumentCompressor):\n",
        "    # model_name:str = 'bge_reranker_large_model_path'\n",
        "    \"\"\"Model name to use for reranking.\"\"\"\n",
        "    top_n: int = 10\n",
        "    \"\"\"Number of documents to return.\"\"\"\n",
        "    model_name = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
        "    model:CrossEncoder = CrossEncoder(model_name)\n",
        "    \"\"\"CrossEncoder instance to use for reranking.\"\"\"\n",
        "\n",
        "    def bge_rerank(self,query,docs):\n",
        "        model_inputs =  [[query, doc] for doc in docs]\n",
        "        scores = self.model.predict(model_inputs)\n",
        "        results = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
        "        return results[:self.top_n]\n",
        "\n",
        "\n",
        "    class Config:\n",
        "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
        "\n",
        "        extra = Extra.forbid\n",
        "        arbitrary_types_allowed = True\n",
        "\n",
        "    def compress_documents(\n",
        "        self,\n",
        "        documents: Sequence[Document],\n",
        "        query: str,\n",
        "        callbacks: Optional[Callbacks] = None,\n",
        "    ) -> Sequence[Document]:\n",
        "        \"\"\"\n",
        "        Compress documents using BAAI/bge-reranker models.\n",
        "\n",
        "        Args:\n",
        "            documents: A sequence of documents to compress.\n",
        "            query: The query to use for compressing the documents.\n",
        "            callbacks: Callbacks to run during the compression process.\n",
        "\n",
        "        Returns:\n",
        "            A sequence of compressed documents.\n",
        "        \"\"\"\n",
        "        if len(documents) == 0:  # to avoid empty api call\n",
        "            return []\n",
        "        doc_list = list(documents)\n",
        "        _docs = [d.page_content for d in doc_list]\n",
        "        results = self.bge_rerank(query, _docs)\n",
        "        final_results = []\n",
        "        for r in results:\n",
        "            doc = doc_list[r[0]]\n",
        "            doc.metadata[\"relevance_score\"] = r[1]\n",
        "            final_results.append(doc)\n",
        "        return final_results\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "# from langchain.retrievers.document_compressors import CohereRerank\n",
        "\n",
        "compressor = BgeRerank()\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")\n",
        "\n",
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "# bm25_retriever = BM25Retriever.from_texts(\n",
        "#     doc_list_1, metadatas=[{\"source\": 1}] * len(doc_list_1)\n",
        "# )\n",
        "# bm25_retriever = BM25Retriever.from_documents(docs)\n",
        "\n",
        "# bm25_retriever.k = 10\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[\n",
        "        compression_retriever,\n",
        "        # RAG_retriever,\n",
        "        # bm25_retriever,\n",
        "        retriever], weights=[0.5, 0.5]\n",
        ")\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\", output_key=\"answer\", return_messages=True\n",
        ")\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    # db.as_retriever(search_kwargs={\"k\": 5}),\n",
        "    # retriever,\n",
        "    ensemble_retriever,\n",
        "    return_source_documents=True,\n",
        "    memory=memory,\n",
        "    verbose=False,\n",
        "    combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
        ")\n",
        "\n",
        "question = \"Was ist die Liste der abgedeckten und nicht abgedeckten Immobilien?\"\n",
        "\n",
        "result = qa_chain.invoke({\"question\": question})\n",
        "print(\"Answer from LLM:\")\n",
        "print(\"================\")\n",
        "print(result[\"answer\"])\n",
        "\n",
        "source_docs = result[\"source_documents\"]\n",
        "print(\"================\")\n",
        "print(f\"Number of used source document chunks: {len(source_docs)}\")\n",
        "\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever,\n",
        "                                        #    return_source_documents=True\n",
        "                                           )\n",
        "\n",
        "qa({\"question\": \"what are the amended exclusions?\", \"chat_history\": {}},\n",
        "#    return_only_outputs=True\n",
        "   )['answer']\n",
        "\n",
        "from langchain.agents import AgentType, Tool, initialize_agent\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
        ")\n",
        "agent.run(\n",
        "    \"what is the summarization of the document?\"\n",
        ")\n",
        "\n",
        "\n",
        "template = \"\"\"\n",
        "Answer the following questions by running the entire PDF attached/mentioned document\n",
        "Please return the summarized results by running the summarization engine\n",
        "\n",
        "Also return the responses/answers for the questions to be asked.\n",
        "\n",
        "You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question for which you must provide a natural language answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Question: {input}\n",
        "{agent_scratchpad}\"\"\"\n",
        "\n",
        "\n",
        "import re\n",
        "from typing import List, Union\n",
        "\n",
        "from langchain.agents import (\n",
        "    AgentExecutor,\n",
        "    AgentOutputParser,\n",
        "    LLMSingleActionAgent,\n",
        "    Tool,\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import StringPromptTemplate\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "\n",
        "\n",
        "# Set up a prompt template\n",
        "class CustomPromptTemplate(StringPromptTemplate):\n",
        "    # The template to use\n",
        "    template: str\n",
        "    # The list of tools available\n",
        "    tools: List[Tool]\n",
        "\n",
        "    def format(self, **kwargs) -> str:\n",
        "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
        "        # Format them in a particular way\n",
        "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
        "        thoughts = \"\"\n",
        "        for action, observation in intermediate_steps:\n",
        "            thoughts += action.log\n",
        "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
        "        # Set the agent_scratchpad variable to that value\n",
        "        kwargs[\"agent_scratchpad\"] = thoughts\n",
        "        # Create a tools variable from the list of tools provided\n",
        "        kwargs[\"tools\"] = \"\\n\".join(\n",
        "            [f\"{tool.name}: {tool.description}\" for tool in self.tools]\n",
        "        )\n",
        "        # Create a list of tool names for the tools provided\n",
        "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
        "        return self.template.format(**kwargs)\n",
        "prompt = CustomPromptTemplate(\n",
        "    template=template,\n",
        "    tools=tools,\n",
        "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
        "    # This includes the `intermediate_steps` variable because that is needed\n",
        "    input_variables=[\"input\", \"intermediate_steps\"],\n",
        ")\n",
        "\n",
        "# class CustomOutputParser(AgentOutputParser):\n",
        "#     def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "#         # Check if agent should finish\n",
        "#         if \"Final Answer:\" in llm_output:\n",
        "#             return AgentFinish(\n",
        "#                 # Return values is generally always a dictionary with a single `output` key\n",
        "#                 # It is not recommended to try anything else at the moment :)\n",
        "#                 return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
        "#                 log=llm_output,\n",
        "#             )\n",
        "#         # Parse out the action and action input\n",
        "#         regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n",
        "#         match = re.search(regex, llm_output, re.DOTALL)\n",
        "#         if not match:\n",
        "#             raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
        "#         action = match.group(1).strip()\n",
        "#         action_input = match.group(2)\n",
        "#         # Return the action and action input\n",
        "#         return AgentAction(\n",
        "#             tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output\n",
        "#         )\n",
        "\n",
        "class CustomOutputParser(AgentOutputParser):\n",
        "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "        # Check if agent should finish\n",
        "        if \"Final Answer:\" in llm_output:\n",
        "            return AgentFinish(\n",
        "                # Return values is generally always a dictionary with a single `output` key\n",
        "                # It is not recommended to try anything else at the moment :)\n",
        "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
        "                log=llm_output,\n",
        "            )\n",
        "        # Parse out the action and action input\n",
        "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
        "        match = re.search(regex, llm_output, re.DOTALL)\n",
        "        if not match:\n",
        "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
        "        action = match.group(1).strip()\n",
        "        action_input = match.group(2)\n",
        "        # Return the action and action input\n",
        "        return AgentAction(\n",
        "            tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output\n",
        "        )\n",
        "\n",
        "output_parser = CustomOutputParser()\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "tool_names = [tool.name for tool in tools]\n",
        "agent = LLMSingleActionAgent(\n",
        "    llm_chain=llm_chain,\n",
        "    output_parser=output_parser,\n",
        "    stop=[\"\\nObservation:\"],\n",
        "    allowed_tools=tool_names,\n",
        ")\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(\n",
        "    agent=agent, tools=tools, verbose=True\n",
        ")\n",
        "\n",
        "agent_executor.run(\"what are the exclusions mentioned in the document?\")"
      ],
      "metadata": {
        "id": "2Qbpv8bli5KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.preprocessing import normalize\n",
        "from transformers.pipelines import Pipeline\n",
        "\n",
        "from keybert.backend import BaseEmbedder\n",
        "\n",
        "\n",
        "class HFTransformerBackend(BaseEmbedder):\n",
        "\n",
        "    def __init__(self, embedding_model: Pipeline):\n",
        "        super().__init__()\n",
        "\n",
        "        if isinstance(embedding_model, Pipeline):\n",
        "            self.embedding_model = embedding_model\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Please select a correct transformers pipeline\"\n",
        "            )\n",
        "\n",
        "    def embed(self, documents: List[str], verbose: bool = False) -> np.ndarray:\n",
        "        dataset = MyDataset(documents)\n",
        "\n",
        "        embeddings = []\n",
        "        for document, features in tqdm(\n",
        "            zip(\n",
        "                documents, self.embedding_model(dataset, truncation=True, padding=True)\n",
        "            ),\n",
        "            total=len(dataset),\n",
        "            disable=not verbose,\n",
        "        ):\n",
        "            embeddings.append(self._embed(document, features))\n",
        "\n",
        "        return np.array(embeddings)\n",
        "\n",
        "    def _embed(self, document: str, features: np.ndarray) -> np.ndarray:\n",
        "        token_embeddings = np.array(features)\n",
        "        attention_mask = self.embedding_model.tokenizer(\n",
        "            document, truncation=True, padding=True, return_tensors=\"np\"\n",
        "        )[\"attention_mask\"]\n",
        "        input_mask_expanded = np.broadcast_to(\n",
        "            np.expand_dims(attention_mask, -1), token_embeddings.shape\n",
        "        )\n",
        "        sum_embeddings = np.sum(token_embeddings * input_mask_expanded, 1)\n",
        "        sum_mask = np.clip(\n",
        "            input_mask_expanded.sum(1),\n",
        "            a_min=1e-9,\n",
        "            a_max=input_mask_expanded.sum(1).max(),\n",
        "        )\n",
        "        embedding = normalize(sum_embeddings / sum_mask)[0]\n",
        "        return embedding\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    \"\"\"Dataset to pass to `transformers.pipelines.pipeline`\"\"\"\n",
        "\n",
        "    def __init__(self, docs):\n",
        "        self.docs = docs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.docs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.docs[idx]\n",
        "\n",
        "from transformers.pipelines import pipeline\n",
        "from keybert import KeyBERT\n",
        "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "hf_model = pipeline(\"feature-extraction\", model=model_id)\n",
        "embedding_model = HFTransformerBackend(hf_model)\n",
        "kw_model = KeyBERT(model=embedding_model)\n",
        "keywords = kw_model.extract_keywords(\"important keywords in the phrase\")\n",
        "keywords\n",
        "\n",
        "kw_model = KeyBERT()\n",
        "keywords = kw_model.extract_keywords(\"important keywords in the phrase\")\n",
        "keywords\n",
        "\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTModelForCustomTasks\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = ORTModelForCustomTasks.from_pretrained(model_id,export=True)\n",
        "model.save_pretrained(\"extracted/\")\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from optimum.onnxruntime import ORTModelForCustomTasks\n",
        "model = ORTModelForCustomTasks.from_pretrained(\"extracted/\")\n",
        "onnx_extractor = pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer,accelerator=\"ort\")\n",
        "embedding_model = HFTransformerBackend(onnx_extractor)\n",
        "kw_model = KeyBERT(model=embedding_model)\n",
        "keywords = kw_model.extract_keywords(\"important keywords in the phrase\")\n",
        "keywords\n"
      ],
      "metadata": {
        "id": "dIJ4fPZ4i5Hs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}